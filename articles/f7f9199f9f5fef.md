---
title: "PCAã®æ•°ç†è§£èª¬ã¨å®Ÿè£…(PyTorch)"
emoji: "ğŸ"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["AI", "æ©Ÿæ¢°å­¦ç¿’", "ç·šå½¢ä»£æ•°", "Python", "PyTorch", "æ¬¡å…ƒå‰Šæ¸›"]
published: false
---

## ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã®æ•°ç†è§£èª¬

PCAã‚’ãŒã£ã¤ã‚Šèª¬æ˜ã—ã¦ã‚‹è¨˜äº‹ã‚’ã‚ã¾ã‚Šè¦‹ãªã„ã®ã§ï¼ˆéœ€è¦ãŒãªã„...ï¼‰PCAã®Pytorchå®Ÿè£…ï¼ˆGPUå¯¾å¿œï¼‰ã®ã¤ã„ã§ã«ã‚„ã£ã¦ã¿ã¾ã—ãŸã€‚æœ¬ç¨¿ã¯åˆæŠ•ç¨¿ã¨ãªã‚Šã¾ã™ãŒã€ä»Šå¾Œã‚‚ã“ã‚“ãªæ„Ÿã˜ã§å‹‰å¼·ã¨å®Ÿè£…ã‚’ç¶™ç¶šã—ã¦ã§ããŸã‚‰ãªï½ã¨æ€ã„ã¾ã™ã€‚

### 0. è¨˜å·ã®å®šç¾©

#### 0.1 æ¬¡å…ƒãƒ»æ·»å­—

* $N\in\mathbb{N}$ ï¼šã‚µãƒ³ãƒ—ãƒ«æ•°
* $d\in\mathbb{N}$ ï¼šç‰¹å¾´æ¬¡å…ƒ
* $k\in\mathbb{N}$ ï¼šä¿æŒã™ã‚‹ä¸»æˆåˆ†æ•°ï¼ˆ $1\le k\le \min(N,d)$ ï¼‰
* æ·»å­—ï¼šã‚µãƒ³ãƒ—ãƒ« $i\in\{1,\dots,N\}$ ã€ç‰¹å¾´ $j\in\{1,\dots,d\}$ ã€ä¸»æˆåˆ† $m\in\{1,\dots,k\}$

#### 0.2 ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—

* ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—

$$
\mathbf{X}\in\mathbb{R}^{N\times d},
\qquad
\mathbf{X}=
\begin{bmatrix}
\mathbf{x}_1^\top\\
\vdots\\
\mathbf{x}_N^\top
\end{bmatrix}
$$

  ã“ã“ã§ $\mathbf{x}_i\in\mathbb{R}^{d}$ ã¯ç¬¬ $i$ ã‚µãƒ³ãƒ—ãƒ«ï¼ˆåˆ—ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã§ã‚ã‚‹ã€‚
* å…¨1ãƒ™ã‚¯ãƒˆãƒ«

$$
\mathbf{1}_N\in\mathbb{R}^{N}
$$

* å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆåˆ—å¹³å‡ï¼‰

$$
\boldsymbol{\mu}\in\mathbb{R}^{d}
$$

* ä¸­å¿ƒåŒ–å¾Œãƒ‡ãƒ¼ã‚¿

$$
\mathbf{X}_c\in\mathbb{R}^{N\times d}
$$

* å…±åˆ†æ•£è¡Œåˆ—ï¼ˆåˆ†æ¯ $N$ ã‚’æ¡ç”¨ï¼‰

$$
\mathbf{S}\in\mathbb{R}^{d\times d}
$$

* å›ºæœ‰ç›´äº¤åˆ†è§£

$$
\mathbf{S}=\mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^\top
$$

* ç‰¹ç•°å€¤åˆ†è§£

$$
\mathbf{X}_c=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top
$$

* ä¸»æˆåˆ†æ–¹å‘ï¼ˆloadingsï¼‰

$$
\mathbf{W}\in\mathbb{R}^{d\times k}
$$

* ä¸»æˆåˆ†å¾—ç‚¹ï¼ˆscoresï¼‰

$$
\mathbf{Z}\in\mathbb{R}^{N\times k}
$$

* èª¬æ˜åˆ†æ•£ï¼ˆä¸»æˆåˆ†åˆ†æ•£ï¼‰

$$
\lambda_m\quad(m=1,\dots,k)
$$

* èª¬æ˜åˆ†æ•£æ¯”ï¼ˆä¿æŒã—ãŸ $k$ æˆåˆ†å†…ã§æ­£è¦åŒ–ï¼‰

$$
\mathrm{EVR}_m:=\frac{\lambda_m}{\sum_{j=1}^k\lambda_j}
$$

---

### 1. ä¸­å¿ƒåŒ–ï¼ˆå¹³å‡ã®é™¤å»ï¼‰

PCAã¯ã€Œå¹³å‡ã‹ã‚‰ã®ã°ã‚‰ã¤ãã€ã‚’æ‰±ã†ãŸã‚ã€å„ç‰¹å¾´ã‚’ä¸­å¿ƒåŒ–ã™ã‚‹ã€‚

å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’

$$
\boldsymbol{\mu}:=\frac{1}{N}\sum_{i=1}^{N}\mathbf{x}_i\in\mathbb{R}^{d}
$$

ã¨å®šç¾©ã™ã‚‹ã€‚è¡Œåˆ—è¡¨è¨˜ã§ã¯

$$
\boldsymbol{\mu}=\frac{1}{N}\mathbf{X}^\top\mathbf{1}_N.
$$

ä¸­å¿ƒåŒ–ãƒ‡ãƒ¼ã‚¿ã¯

$$
\mathbf{X}_c:=\mathbf{X}-\mathbf{1}_N\boldsymbol{\mu}^\top\in\mathbb{R}^{N\times d}
$$

ã§ã‚ã‚‹ã€‚

---

### 2. å…±åˆ†æ•£è¡Œåˆ—ã¨ãã®æ€§è³ª

å…±åˆ†æ•£è¡Œåˆ—ã‚’

$$
\mathbf{S}:=\frac{1}{N}\mathbf{X}_c^\top\mathbf{X}_c\in\mathbb{R}^{d\times d}\tag{2.1}
$$

ã¨å®šç¾©ã™ã‚‹ï¼ˆåˆ†æ¯ $N$ ï¼‰ã€‚

#### 2.1 å¯¾ç§°æ€§

$$
\mathbf{S}^\top=\left(\frac{1}{N}\mathbf{X}_c^\top\mathbf{X}_c\right)^\top=\frac{1}{N}\mathbf{X}_c^\top\mathbf{X}_c=\mathbf{S}.
$$

å¯¾ç§°è¡Œåˆ—ã§ã‚ã‚Œã°ã€ç•°ãªã‚‹å›ºæœ‰å€¤ã«å¯¾å¿œã™ã‚‹å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã¯å¿…ãšç›´äº¤ã™ã‚‹ã€‚

#### 2.2 åŠæ­£å®šå€¤æ€§ï¼ˆPSDï¼‰

ä»»æ„ã® $\mathbf{a}\in\mathbb{R}^d$ ã«å¯¾ã—ã¦

$$
\mathbf{a}^\top\mathbf{S}\mathbf{a}=\frac{1}{N}\mathbf{a}^\top\mathbf{X}_c^\top\mathbf{X}_c\mathbf{a}=\frac{1}{N}\|\mathbf{X}_c\mathbf{a}\|_2^2\ge 0.
$$

å¾“ã£ã¦ $\mathbf{S}$ ã¯å¯¾ç§°åŠæ­£å®šå€¤ã§ã‚ã‚‹ï¼ˆåˆ†æ•£ã‚„ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒãƒã‚¤ãƒŠã‚¹ã«ãªã‚‹ã“ã¨ã¯ãªã„ãŸã‚ã€åŠæ­£å®šå€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰ã€‚

ã“ã®æ€§è³ªã‹ã‚‰ã€$\mathbf{S}$ ã¯å›ºæœ‰ç›´äº¤åˆ†è§£

$$
\mathbf{S}=\mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^\top,
\qquad
\mathbf{Q}^\top\mathbf{Q}=\mathbf{I},
\qquad
\boldsymbol{\Lambda}=\mathrm{diag}(\lambda^\ast_1,\dots,\lambda^\ast_d),
\quad
\lambda^\ast_1\ge\dots\ge\lambda^\ast_d\ge 0
\tag{2.2}
$$

ã‚’æŒã¤ã€‚

---

### 3. PCAã®å®šå¼åŒ–ï¼šç­‰ä¾¡ãª2ã¤ã®æœ€é©åŒ–å•é¡Œ

PCAã¯ä»£è¡¨çš„ã«æ¬¡ã®2ã¤ã®å•é¡Œã¨ã—ã¦å®šç¾©ã§ãã€ã“ã‚Œã‚‰ã¯ç­‰ä¾¡ã§ã‚ã‚‹ã€‚

#### 3.1 å°„å½±å†æ§‹æˆèª¤å·®ã®æœ€å°åŒ–ï¼ˆæœ€å°äºŒä¹—ï¼‰

$\mathbf{W}\in\mathbb{R}^{d\times k}$ ãŒåˆ—ç›´äº¤

$$
\mathbf{W}^\top\mathbf{W}=\mathbf{I}_k
\tag{3.1}
$$


ã‚’æº€ãŸã™ã¨ã™ã‚‹ã€‚å°„å½±è¡Œåˆ—ã‚’

$$
\mathbf{P}:=\mathbf{W}\mathbf{W}^\top
\in\mathbb{R}^{d\times d}
\tag{3.2}
$$

ã¨å®šç¾©ã™ã‚‹ã¨ã€ä¸­å¿ƒåŒ–ãƒ‡ãƒ¼ã‚¿ã®å°„å½±å†æ§‹æˆã¯

$$
\widehat{\mathbf{X}}_c=\mathbf{X}_c\mathbf{P}=\mathbf{X}_c\mathbf{W}\mathbf{W}^\top.
\tag{3.3}
$$

ã“ã®ã¨ã PCA ã¯

$$
\min_{\mathbf{W}}\
\left\|\mathbf{X}_c-\mathbf{X}_c\mathbf{W}\mathbf{W}^\top\right\|_F^2
\quad\text{s.t.}\quad
\mathbf{W}^\top\mathbf{W}=\mathbf{I}_k
\tag{P1}
$$

ã‚’è§£ãã“ã¨ã¨ã—ã¦å®šç¾©ã§ãã‚‹ã€‚

#### 3.2 å°„å½±å¾Œåˆ†æ•£ã®æœ€å¤§åŒ–

ã‚¹ã‚³ã‚¢ï¼ˆä½æ¬¡å…ƒè¡¨ç¾ï¼‰ã‚’

$$
\mathbf{Z}:=\mathbf{X}_c\mathbf{W}\in\mathbb{R}^{N\times k}\tag{3.4}
$$

ã¨å®šç¾©ã™ã‚‹ã€‚ã™ã‚‹ã¨

$$
\mathbf{Z}^\top\mathbf{Z}=\mathbf{W}^\top\mathbf{X}_c^\top\mathbf{X}_c\mathbf{W}.\tag{3.5}
$$

ã‚ˆã£ã¦ $\mathbf{S}=\frac{1}{N}\mathbf{X}_c^\top\mathbf{X}_c$ ã‚ˆã‚Šã€ã‚¹ã‚³ã‚¢ã®ç·åˆ†æ•£ã¯

$$
\frac{1}{N}\mathrm{tr}(\mathbf{Z}^\top\mathbf{Z})=
\mathrm{tr}\left(\mathbf{W}^\top\mathbf{S}\mathbf{W}\right).
\tag{3.6}
$$

å¾“ã£ã¦ PCA ã¯

$$
\max_{\mathbf{W}}\
\mathrm{tr}\left(\mathbf{W}^\top\mathbf{S}\mathbf{W}\right)
\quad\text{s.t.}\quad
\mathbf{W}^\top\mathbf{W}=\mathbf{I}_k
\tag{P2}
$$

ã¨ã—ã¦ã‚‚è¡¨ã›ã‚‹ï¼ˆKy Fan ã®æœ€å¤§åŒ–å•é¡Œï¼‰ã€‚

---

### 4. (P1) ã¨ (P2) ã®ç­‰ä¾¡æ€§

(P1) ã®ç›®çš„é–¢æ•°ã‚’å±•é–‹ã™ã‚‹ã€‚ã¾ãš Frobenius ãƒãƒ«ãƒ ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã§æ›¸ãï¼š

$$
\|\mathbf{A}\|_F^2=\mathrm{tr}(\mathbf{A}^\top\mathbf{A}).
$$

$\mathbf{A}:=\mathbf{X}_c-\mathbf{X}_c\mathbf{P}$ ã¨ç½®ã‘ã°

$$
\|\mathbf{X}_c-\mathbf{X}_c\mathbf{P}\|_F^2=\mathrm{tr}\left((\mathbf{X}_c-\mathbf{X}_c\mathbf{P})^\top(\mathbf{X}_c-\mathbf{X}_c\mathbf{P})\right).
\tag{4.1}
$$


å³è¾ºã‚’å±•é–‹ã™ã‚‹ã¨

$$
(\mathbf{X}_c-\mathbf{X}_c\mathbf{P})^\top(\mathbf{X}_c-\mathbf{X}_c\mathbf{P})=\mathbf{X}_c^\top\mathbf{X}_c-\mathbf{X}_c^\top\mathbf{X}_c\mathbf{P}-\mathbf{P}^\top\mathbf{X}_c^\top\mathbf{X}_c+\mathbf{P}^\top\mathbf{X}_c^\top\mathbf{X}_c\mathbf{P}.\tag{4.2}
$$


ã“ã“ã§ $\mathbf{P}=\mathbf{W}\mathbf{W}^\top$ ã‹ã¤ $\mathbf{W}^\top\mathbf{W}=\mathbf{I}_k$ ã‚ˆã‚Š

$$
\mathbf{P}^\top=\mathbf{P},\qquad
\mathbf{P}^2=\mathbf{P}
\tag{4.3}
$$

ãŒæˆã‚Šç«‹ã¤ï¼ˆå¯¾ç§°ã‹ã¤ã¹ãç­‰ï¼‰ã€‚
ã•ã‚‰ã«(4.3)ã¨ $\mathrm{tr}(AB)=\mathrm{tr}(BA)$ ã‚’ç”¨ã„ã‚‹ã¨ã€

$$
\mathrm{tr}(\mathbf{P}^\top\mathbf{X}_c^\top\mathbf{X}_c)=\mathrm{tr}(\mathbf{X}_c^\top\mathbf{X}_c\mathbf{P})
\tag{4.4}
$$

ã§ã‚ã‚‹ã€‚ã¾ãŸ (4.3) ã‚ˆã‚Š

$$
\mathrm{tr}(\mathbf{P}^\top\mathbf{X}_c^\top\mathbf{X}_c\mathbf{P})=\mathrm{tr}(\mathbf{X}_c^\top\mathbf{X}_c\mathbf{P}^2)=\mathrm{tr}(\mathbf{X}_c^\top\mathbf{X}_c\mathbf{P}).
\tag{4.5}
$$

(4.1)ã€œ(4.5) ã‚’åˆã‚ã›ã‚‹ã¨

$$
\begin{aligned}
\|\mathbf{X}_c-\mathbf{X}_c\mathbf{P}\|_F^2&=
\mathrm{tr}(\mathbf{X}_c^\top\mathbf{X}_c) - \mathrm{tr}(\mathbf{X}_c^\top\mathbf{X}_c\mathbf{P})-\mathrm{tr}(\mathbf{P}^\top\mathbf{X}_c^\top\mathbf{X}_c)+\mathrm{tr}(\mathbf{P}^\top\mathbf{X}_c^\top\mathbf{X}_c\mathbf{P})\\
&=\mathrm{tr}(\mathbf{X}_c^\top\mathbf{X}_c)
-\mathrm{tr}(\mathbf{P}\mathbf{X}_c^\top\mathbf{X}_c).
\tag{4.6}
\end{aligned}
$$

$\mathbf{S}=\frac{1}{N}\mathbf{X}_c^\top\mathbf{X}_c$ ã‚’ä»£å…¥ã—ã€

$$
\mathrm{tr}(\mathbf{P}\mathbf{X}_c^\top\mathbf{X}_c)
=N\mathrm{tr}(\mathbf{P}\mathbf{S})
=N\mathrm{tr}(\mathbf{W}\mathbf{W}^\top\mathbf{S})
=N\mathrm{tr}(\mathbf{W}^\top\mathbf{S}\mathbf{W})
\tag{4.7}
$$

ã‚ˆã‚Šã€

$$
\|\mathbf{X}_c-\mathbf{X}_c\mathbf{W}\mathbf{W}^\top\|_F^2=
\mathrm{tr}(\mathbf{X}_c^\top\mathbf{X}_c)-N\mathrm{tr}(\mathbf{W}^\top\mathbf{S}\mathbf{W}).\tag{4.8}
$$

ã“ã“ã§ $\mathrm{tr}(\mathbf{X}_c^\top\mathbf{X}_c)$ ã¯ $\mathbf{W}$ ã«ä¾ã‚‰ãªã„å®šæ•°ãªã®ã§ã€(P1) ã®æœ€å°åŒ–ã¯

$$
\max_{\mathbf{W}}
\mathrm{tr}(\mathbf{W}^\top\mathbf{S}\mathbf{W})
\tag{4.9}
$$

ã¨ç­‰ä¾¡ã§ã‚ã‚Šã€ã“ã‚Œã¯ (P2) ã§ã‚ã‚‹ã€‚

---

### 5.å›ºæœ‰å€¤åˆ†è§£ã‚’ç”¨ã„ãŸè§£æ³•

$\mathbf{S}$ ã®å›ºæœ‰åˆ†è§£ (2.2) ã‚’ç”¨ã„ã‚‹ã€‚åˆ¶ç´„ $\mathbf{W}^\top\mathbf{W}=\mathbf{I}_k$ ã®ä¸‹ã§

$$
\mathrm{tr}(\mathbf{W}^\top\mathbf{S}\mathbf{W})=
\mathrm{tr}(\mathbf{W}^\top\mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^\top\mathbf{W}).
\tag{5.1}
$$

$\mathbf{Y}:=\mathbf{Q}^\top\mathbf{W}\in\mathbb{R}^{d\times k}$ ã¨ãŠãã¨ã€ $\mathbf{S}$ ãŒæ­£æ–¹è¡Œåˆ—ã§ã‚ã‚‹ãŸã‚ $\mathbf{Q}$ ã‚‚æ­£æ–¹è¡Œåˆ—ã§ã‚ã‚Šã€$\mathbf{Q}^\top\mathbf{Q}=\mathbf{I}$ ã§ã‚ã‚‹ãŸã‚ã€

$$
\mathbf{Y}^\top\mathbf{Y}=\mathbf{W}^\top\mathbf{Q}\mathbf{Q}^\top\mathbf{W}
=\mathbf{W}^\top\mathbf{W}=\mathbf{I}_k,
\tag{5.2}
$$


ã‹ã¤ (5.1) ã¯

$$
\begin{aligned}
\mathrm{tr}(\mathbf{W}^\top\mathbf{S}\mathbf{W})
&=\mathrm{tr}(\mathbf{Y}^\top\boldsymbol{\Lambda}\mathbf{Y})\\
&=\sum_{m=1}^{k}\mathbf{y}_m^\top\boldsymbol{\lambda}\mathbf{y}_m \\
&=\sum_{m=1}^{k}\sum_{j=1}^{d}\lambda^\ast_j y_{j m}^2.
\tag{5.3}
\end{aligned}
$$

$\boldsymbol{\lambda}=\begin{pmatrix}\lambda_{1} & & 0 \\ & \ddots & \\ 0 & & \lambda_{d}\end{pmatrix}$ ã‚ˆã‚ŠçœŸã‚“ä¸­ã® $\boldsymbol{\lambda}$ ã‚’ã°ã‚‰ã—ã¦è¡Œåˆ—ã®æ›ã‘ç®—ã‚’è¶³ã—ç®—ã«æ›¸ãæ›ãˆã¦ã„ã‚‹ã€‚


ã“ã“ã§ $\sum_{j=1}^{d}y_{j m}^2=1$ï¼ˆå„åˆ—ãƒãƒ«ãƒ 1ï¼‰ã‹ã¤åˆ—ç›´äº¤ã®åˆ¶ç´„ãŒã‚ã‚‹ã€‚å›ºæœ‰å€¤ãŒé™é †ã§ã‚ã‚‹ã“ã¨ã‹ã‚‰ã€(5.3) ã¯é‡ã¿ $y_{jm}^2$ ã‚’å¤§ãã„å›ºæœ‰å€¤ã«é›†ä¸­ã•ã›ã‚‹ã»ã©å¤§ãããªã‚‹ã€‚æœ€å¤§å€¤ã¯

$$
\max\mathrm{tr}(\mathbf{W}^\top\mathbf{S}\mathbf{W})=\sum_{j=1}^{k}\lambda^\ast_j\tag{5.4}
$$

ã§é”æˆã•ã‚Œã‚‹ã€‚ã¤ã¾ã‚Šæœ€åˆã® $k$ å€‹ã®å¤§ããªå›ºæœ‰å€¤ã‚’å…¨éƒ¨è¶³ã—ãŸã‚‚ã®ãŒã€å–ã‚Šå‡ºã›ã‚‹æƒ…å ±ã®æœ€å¤§é‡ã§ã‚ã‚‹ã€‚

ã“ã®ã¨ãã€ $\mathbf{Y}=\begin{pmatrix}\mathbf{I}_k\\\mathbf{0}\end{pmatrix}$ ã§ã‚ã‚Šã€ $\mathbf{Y}=\mathbf{Q}^\top\mathbf{W}$ ã®ä¸¡è¾ºã«å·¦ã‹ã‚‰ $\mathbf{Q}$ ã‚’ã‹ã‘ã‚‹ã¨ã€

$$
\begin{aligned}
\mathbf{W}&=\mathbf{Q}\mathbf{Y}\\
&=\left[\mathbf{q}_1, \mathbf{q}_2, \ldots, \mathbf{q}_d\right]\begin{pmatrix}\mathbf{I}_k\\\mathbf{0}\end{pmatrix}\\
&=[\mathbf{q}_1,\dots,\mathbf{q}_k]
\end{aligned}
$$

ã‚ˆã£ã¦ã€ $\mathbf{W}$ ã¯ å…±åˆ†æ•£è¡Œåˆ— $\mathbf{S}$ ã®ä¸Šä½ $k$ å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸¦ã¹ãŸã‚‚ã®ã«ãªã‚‹ã€‚

ã¤ã¾ã‚ŠPCAã®æœ€é©åŒ–å•é¡Œã®è§£ã¯ $\mathbf{W}=\mathbf{Q}_k$ ã§ã‚ã‚‹ã€‚

---

### 6. SVDã‚’ç”¨ã„ãŸè§£æ³•

ä¸­å¿ƒåŒ–è¡Œåˆ— $\mathbf{X}_c$ ã® thin SVD ã‚’

$$
\mathbf{X}_c=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top
\tag{6.1}
$$

ã¨ã™ã‚‹ã€‚
ã“ã®ã¨ãã€ $r$ ã‚’ $\mathbf{X}_c$ ã®ãƒ©ãƒ³ã‚¯ã¨ã™ã‚‹ã¨ã€
- $\mathbf{U}\in\mathbb{R}^{N\times r}$ : å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ã§ã‚ã‚Šã€ $\mathbf{U}^\top\mathbf{U}=\mathbf{I}_r$ ã‚’æº€ãŸã™ã€‚
- $\mathbf{\Sigma}\in\mathbb{R}^{r\times r}$ : ç‰¹ç•°å€¤è¡Œåˆ—ã€‚å¯¾è§’æˆåˆ†ã«ã¯æ­£ã®ç‰¹ç•°å€¤ $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_r>0$ ãŒé™é †ã«ä¸¦ã‚“ã§ã„ã‚‹ã€‚
- $\mathbf{V}\in\mathbb{R}^{d\times r}$ : å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ã§ã‚ã‚Šã€ $\mathbf{V}^\top\mathbf{V}=\mathbf{I}_r$ ã‚’æº€ãŸã™ã€‚ã“ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ãŒPCAã«ãŠã‘ã‚‹ä¸»æˆåˆ†ã®æ–¹å‘ï¼ˆå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«å¯¾å¿œã™ã‚‹ã€‚


ã“ã‚Œã‚’ç”¨ã„ã¦å…±åˆ†æ•£ã‚’è¨ˆç®—ã™ã‚‹ã¨

$$
\begin{aligned}
\mathbf{S}&=\frac{1}{N}\mathbf{X}_c^\top\mathbf{X}_c\\
&=\frac{1}{N}(\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top)^\top(\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top)\\
&=\frac{1}{N}\mathbf{V}\boldsymbol{\Sigma}\mathbf{U}^\top\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top\\
&=\frac{1}{N}\mathbf{V}\boldsymbol{\Sigma}^2\mathbf{V}^\top.
\end{aligned}
\tag{6.2}
$$

ã“ã®ä¸¡è¾ºã«å³å´ã‹ã‚‰ $\mathbf{V}$ ã‚’ã‹ã‘ã‚‹ã¨

$$
\begin{aligned}
\mathbf{S}\mathbf{V}&=\frac{1}{N}\mathbf{V}\boldsymbol{\Sigma}^2\mathbf{V}^\top\mathbf{V}\\
&=\mathbf{V}\left(\frac{1}{N}\boldsymbol{\Sigma}^2\right).
\end{aligned}
\tag{6.3}
$$

$\mathbf{V}$ ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã®é•·ã•ã¯ 1 ã§ã‚ã‚‹ã‹ã‚‰ã€ $\mathbf{S}$ ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã¯ $\mathbf{V}$ ã®åˆ—ã§ã‚ã‚Šã€å›ºæœ‰å€¤ã¯

$$
\lambda^\ast_m=\frac{\sigma_m^2}{N}
\qquad(m=1,\dots,r).
\tag{6.4}
$$

å›ºæœ‰å€¤ $\lambda^\ast_m$ ã¯ç‰¹ç•°å€¤ $\sigma_m$ ã®2ä¹—ã«æ¯”ä¾‹ã™ã‚‹ã€‚ç‰¹ç•°å€¤ãŒå¤§ãã„é †ã«ä¸¦ã‚“ã§ã„ã‚‹ã¨ã„ã†ã“ã¨ã¯ã€å›ºæœ‰å€¤ã‚‚è‡ªå‹•çš„ã«å¤§ãã„é †ã«ä¸¦ã‚“ã§ã„ã‚‹ã“ã¨ã«ãªã‚‹ã€‚

ã™ãªã‚ã¡ã€ $\mathbf{S}$ ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{V}$ è¡Œåˆ—ã®å·¦ã‹ã‚‰ $k$ åˆ—ç›®ã¾ã§ã®ãƒ™ã‚¯ãƒˆãƒ« $[\mathbf{v}_1, \dots, \mathbf{v}_k]$ ã¯ã€ãã®ã¾ã¾ä¸Šä½ $k$ å€‹ã®å¤§ããªå›ºæœ‰å€¤ã«å¯¾å¿œã™ã‚‹å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾å¿œã™ã‚‹ã“ã¨ãŒåˆ†ã‚‹ã€‚

ã‚»ã‚¯ã‚·ãƒ§ãƒ³5ã®æœ€å¾Œã§ã€PCAã®æœ€é©è§£ $\mathbf{W}$ ã¯å…±åˆ†æ•£è¡Œåˆ— $\mathbf{S}$ ã®å›ºæœ‰å€¤ãŒå¤§ãã„é †ã«å¯¾å¿œã™ã‚‹å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ $k$ æœ¬ä¸¦ã¹ãŸã‚‚ã®ã§ã‚ã‚‹ã¨ç¤ºã—ã¦ã„ã‚‹ãŸã‚ã€
ä¸Šä½ $k$ ä¸»æˆåˆ†æ–¹å‘ã¯

$$
\mathbf{W}=\mathbf{Q}_k=\mathbf{V}_k.
\tag{6.5}
$$

å…±åˆ†æ•£è¡Œåˆ—ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{Q}$ ã¨å…ƒã®ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—ã®å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{V}$ ãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã€‚

**ã¤ã¾ã‚Šã€ç†è«–çš„ãªå®šç¾©ï¼ˆå›ºæœ‰å€¤åˆ†è§£ãƒ™ãƒ¼ã‚¹ï¼‰ã¨ã€å®Ÿç”¨çš„ãªè¨ˆç®—æ³•ï¼ˆSVDãƒ™ãƒ¼ã‚¹ï¼‰ãŒ $\mathbf{W}$ ã¨ã„ã†ä¸€ã¤ã®è§£ã‚’é€šã—ã¦ç¹‹ãŒã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚**ã€€ï¼ˆç¾ã—ã„ã­ï¼ï¼ï¼‰

> ã‚‚ã—ã€æ´»æ€§åŒ–é–¢æ•°ãªã—ã®ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆç·šå½¢å¤‰æ›ã®ã¿ï¼‰ã‚’ä½œã‚Šã€äºŒä¹—èª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ãŸã¨ã—ã¾ã™ã€‚å®Ÿã¯ã“ã®ã¨ãã€ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ä¸­é–“å±¤ï¼ˆãƒœãƒˆãƒ«ãƒãƒƒã‚¯ï¼‰ãŒç²å¾—ã™ã‚‹ä½æ¬¡å…ƒã®éƒ¨åˆ†ç©ºé–“ã¯ã€PCAãŒå¼µã‚‹ä¸»æˆåˆ†ç©ºé–“ã¨æ•°å­¦çš„ã«å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã“ã¨ãŒè¨¼æ˜ã•ã‚Œã¦ã„ã¾ã™ï¼ˆBaldi & Hornik, 1989ï¼‰ã€‚ã¤ã¾ã‚Šã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã€Œç·šå½¢ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«æˆ»ã›ã€ã¨å‘½ä»¤ã™ã‚‹ã¨ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯å‹¾é…é™ä¸‹æ³•ã¨ã„ã†åŠ›æŠ€ã‚’ä½¿ã£ã¦ã€å…ˆã»ã©ç§ãŒå¿…æ­»ã«æ•°å¼ã§å°ã„ãŸ $\mathbf{W}$ ã¨åŒã˜æƒ…å ±ç©ºé–“ã‚’è‡ªåŠ›ã§è¦‹ã¤ã‘å‡ºã™ã®ã§ã™ã€‚

---

### 7. ã‚¹ã‚³ã‚¢ï¼ˆä½æ¬¡å…ƒè¡¨ç¾ï¼‰ã¨ãã®å±•é–‹

ã‚¹ã‚³ã‚¢ã‚’

$$
\mathbf{Z}:=\mathbf{X}_c\mathbf{W}
\in\mathbb{R}^{N\times k}
\tag{7.1}
$$

ã¨å®šç¾©ã™ã‚‹ã€‚ã•ã‚‰ã« $\mathbf{W}=\mathbf{V}_k$ ã¨ SVD (6.1) ã‚’ç”¨ã„ã¦å±•é–‹ã™ã‚‹ï¼š

$$
\mathbf{Z}
=\mathbf{X}_c\mathbf{V}_k
=(\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top)\mathbf{V}_k
=\mathbf{U}\boldsymbol{\Sigma}(\mathbf{V}^\top\mathbf{V}_k).
\tag{7.2}
$$

$\mathbf{V}=\left[\mathbf{V}_k\ \mathbf{V}_\perp\right]$ ã¨åˆ†å‰²ã™ã‚‹ã¨

$$
\mathbf{V}^\top\mathbf{V}_k=
\begin{bmatrix}
\mathbf{V}_k^\top\\
\mathbf{V}_\perp^\top
\end{bmatrix}\mathbf{V}_k=
\begin{bmatrix}
\mathbf{I}_k\\
\mathbf{0}
\end{bmatrix},
\tag{7.3}
$$

$\mathbf{V}$ ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã¯ã™ã¹ã¦äº’ã„ã«ç›´äº¤ã—ã¦ã„ã¦ã€é•·ã•ãŒ1ã®æ­£è¦ç›´äº¤ç³»ã§ã‚ã‚‹ã€‚ã—ãŸãŒã£ã¦ã€
- $\mathbf{V}_k^\top \mathbf{V}_k=\mathbf{I}_k$ 
å‰åŠã‚°ãƒ«ãƒ¼ãƒ—åŒå£«ã®å†…ç©ã§ã‚ã‚‹ã€‚åŒã˜ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®å†…ç©ï¼ˆé•·ã•ï¼‰ã¯ $1$ã€é•ã†ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®å†…ç©ã¯ $0$ ã«ãªã‚‹ãŸã‚ã€å¯¾è§’æˆåˆ†ã ã‘ãŒ $1$ ã®å˜ä½è¡Œåˆ— $\mathbf{I}_k$ ã«ãªã‚‹ã€‚
- $\mathbf{V}_\perp^\top \mathbf{V}_k=\mathbf{0}$
ã€Œå¾ŒåŠã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ™ã‚¯ãƒˆãƒ«ã€ã¨ã€Œå‰åŠã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ™ã‚¯ãƒˆãƒ«ã€ã®å†…ç©ã‚’è¨ˆç®—ã™ã‚‹ã€‚SVDã®æ€§è³ªä¸Šã€ç•°ãªã‚‹åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã¯ä¾‹å¤–ãªãã™ã¹ã¦ç›´äº¤ã—ã¦ã„ã‚‹ï¼ˆ$\mathbf{v}_i^\top \mathbf{v}_j = 0 \quad (i \neq j)$ï¼‰ãŸã‚ã€ã©ã®çµ„ã¿åˆã‚ã›ã§å†…ç©ã‚’ã¨ã£ã¦ã‚‚å¿…ãš $0$ ã«ãªã‚‹ã€‚
çµæœã¨ã—ã¦ã€æˆåˆ†ãŒã™ã¹ã¦ $0$ ã®ã‚¼ãƒ­è¡Œåˆ— $\mathbf{0}$ ã«ãªã‚‹ã€‚

ã‚ˆã£ã¦

$$
\mathbf{Z}=
\mathbf{U}\boldsymbol{\Sigma}
\begin{bmatrix}
\mathbf{I}_k\\
\mathbf{0}
\end{bmatrix}=
\mathbf{U}_k\boldsymbol{\Sigma}_k.
\tag{7.4}
$$

---

### 8. èª¬æ˜åˆ†æ•£ãƒ»èª¬æ˜åˆ†æ•£æ¯”

ç¬¬ $m$ ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢åˆ—ã‚’ $\mathbf{z}_m:=\mathbf{Z}_{[:,m]}$ ã¨ã™ã‚‹ã€‚
ï¼ˆåˆ†æ¯ $N$ ã®ï¼‰åˆ†æ•£ã‚’

$$
\mathrm{Var}_N(\mathbf{z}_m)
:=
\frac{1}{N}\|\mathbf{z}_m\|_2^2
\tag{8.1}
$$

ã¨å®šç¾©ã™ã‚‹ã€‚

(7.4) ã‚ˆã‚Š $\mathbf{z}_m=\sigma_m\mathbf{u}_m$ ( $\|\mathbf{u}_m\|_2=1$ )ãªã®ã§ã€

$$
\mathrm{Var}_N(\mathbf{z}_m)=
\frac{1}{N}|\sigma_m\mathbf{u}_m|_2^2=
\frac{1}{N}\sigma_m^2.
\tag{8.2}
$$

ã“ã“ã§

$$
\lambda_m:=\frac{\sigma_m^2}{N}
\tag{8.3}
$$

ã‚’ç¬¬ $m$ ä¸»æˆåˆ†ã®èª¬æ˜åˆ†æ•£ã¨å‘¼ã¶ã€‚
ã¾ãŸã€èª¬æ˜åˆ†æ•£æ¯”ã‚’ä¿æŒã—ãŸ $k$ æˆåˆ†å†…ã§æ­£è¦åŒ–ã—ã¦

$$
\mathrm{EVR}_m:=\frac{\lambda_m}{\sum_{j=1}^{k}\lambda_j}
\tag{8.4}
$$

ã¨å®šç¾©ã™ã‚‹ã€‚( $N$ ã¯ç´„åˆ†ã§ãã‚‹ã®ã§)

$$
\mathrm{EVR}_m=
\frac{\sigma_m^2}{\sum_{j=1}^k\sigma_j^2}.
\tag{8.5}
$$

---

### 9. ç™½è‰²åŒ–ï¼ˆwhiteningï¼‰

ç™½è‰²åŒ–ã¯å„ä¸»æˆåˆ†ã‚’åˆ†æ•£1ã«æ­£è¦åŒ–ã™ã‚‹æ“ä½œã§ã‚ã‚‹ã€‚ç™½è‰²åŒ–ã‚¹ã‚³ã‚¢ã‚’

$$
\mathbf{Z}^{\mathrm{white}}:=
\mathbf{Z}\mathrm{diag}(\lambda_1^{-1/2},\dots,\lambda_k^{-1/2})
\tag{9.1}
$$

ã§å®šç¾©ã™ã‚‹ã€‚åˆ—ã”ã¨ã«ã¯

$$
\mathbf{z}_{m}^\mathrm{white}=
\frac{\mathbf{z}_m}{\sqrt{\lambda_m}}.
\tag{9.2}
$$

åˆ†æ•£ã‚’ç¢ºã‹ã‚ã‚‹ã€‚(8.1)~(8.3)ã‚ˆã‚Šã€

$$
\mathrm{Var}_N(\mathbf{z}_m^\mathrm{white})=
\frac{1}{N}\left\|\frac{\mathbf{z}_m}{\sqrt{\lambda_m}}\right\|_2^2=
\frac{1}{N}\frac{\|\mathbf{z}_m\|_2^2}{\lambda_m}=
\frac{1}{N}\frac{|\mathbf{z}_m|_2^2}{(1/N)\|\mathbf{z}_m\|_2^2}=
1.
\tag{9.3}
$$

å¾“ã£ã¦ç™½è‰²åŒ–å¾Œã€å„ä¸»æˆåˆ†ã¯åˆ†æ•£ 1 ã‚’æŒã¤ã€‚

---

## PCAã® Pytorch å®Ÿè£…ï¼ˆGPUå¯¾å¿œï¼‰

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

#### 1. ç›®çš„ã¨ä»•æ§˜ï¼ˆã“ã® PCA ã®æµå„€ï¼‰

æœ¬å®Ÿè£…ã¯ **SVDï¼ˆthin SVDï¼‰ãƒ™ãƒ¼ã‚¹**ã§ PCA ã‚’è¡Œã„ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå´ã®è¨˜å·å®šç¾©ã«å¿ å®Ÿãªå½¢ã§å„é‡ã‚’ä¿æŒã™ã‚‹ï¼ˆã¾ãŸã¯å‚ç…§å¯èƒ½ã«ã™ã‚‹ï¼‰ã‚¯ãƒ©ã‚¹ã§ã‚ã‚‹ã€‚ä¸­å¿ƒåŒ–ãƒ»å…±åˆ†æ•£ãƒ»ã‚¹ã‚³ã‚¢ãƒ»èª¬æ˜åˆ†æ•£æ¯”ãƒ»whitening ã‚’ä¸€è²«ã—ãŸå®šç¾©ã§æä¾›ã™ã‚‹ã€‚

* å…¥åŠ›ã¯ `X âˆˆ R^{NÃ—d}` ã® **2æ¬¡å…ƒãƒ»æµ®å‹•å°æ•°ç‚¹ Tensor**ã®ã¿ï¼ˆFail Fastï¼‰ã€‚
* å…±åˆ†æ•£ã¯ **åˆ†æ¯ `N - cov_ddof`**ï¼ˆ`S = (X_c^T X_c)/(N - cov_ddof)`ï¼‰ã€‚

  * `cov_ddof=0` ã¯åˆ†æ¯ `N`ï¼ˆMLE é¢¨ï¼‰
  * `cov_ddof=1` ã¯åˆ†æ¯ `N-1`ï¼ˆsklearn äº’æ›ã®æ¨™æœ¬åˆ†æ•£ã€æ—¢å®šï¼‰
* thin SVD: `X_c = U diag(Ïƒ) V^T` ã‚’ç”¨ã„ã€ä¸»æˆåˆ†æ–¹å‘ã¯ `W = V_k`ã€ã‚¹ã‚³ã‚¢ã¯ `Z = X_c W`ã€‚
* èª¬æ˜åˆ†æ•£ã¯ `Î»_m = Ïƒ_m^2 / (N - cov_ddof)`ã€‚
* whitening ã¯ `Z_white = Z / sqrt(Î»)`ï¼ˆæˆåˆ†ã”ã¨ã€ã“ã“ã§ã® `Î»` ã¯ä¸Šã® ddof è¦ç´„ã«å¾“ã†ï¼‰ã€‚

#### 2. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

##### 2.1 å­¦ç¿’ â†’ å°„å½±ï¼ˆã‚¹ã‚³ã‚¢ï¼‰ã‚’å¾—ã‚‹

```python
import torch
from pca import PCA

X = torch.randn(1000, 64, dtype=torch.float32)

pca = PCA(n_components=16, center=True)  # whiten ã¯æ—¢å®š False, cov_ddof ã¯æ—¢å®š 1
pca.fit(X)

Z = pca.transform(X)          # (1000, 16) éwhiten
Z2 = pca.fit_transform(X)     # fit + transformï¼ˆæ¡ä»¶ã«ã‚ˆã‚Š Z_ ã‚’å†åˆ©ç”¨ï¼‰
```

* `fit_transform()` ã¯ `store_data=True` ã‹ã¤ **whitenã—ãªã„**å ´åˆã€`fit()` å†…ã§è¨ˆç®—æ¸ˆã¿ã® `Z_` ã‚’ãã®ã¾ã¾è¿”ã—ã¦å†è¨ˆç®—ã—ãªã„ã€‚

##### 2.2 å°„å½± â†’ é€†å¤‰æ›ï¼ˆrank-k å†æ§‹æˆï¼‰

```python
X_hat = pca.inverse_transform(Z)  # (1000, 64)
```

* é€†å¤‰æ›ã¯ `X_hat_c = Z W^T`ã€ä¸­å¿ƒåŒ–ã—ã¦ã„ã‚‹å ´åˆã¯ `X_hat = X_hat_c + Î¼`ã€‚

#### 3. Whitening ã®ä½¿ã„æ–¹

##### 3.1 `transform()` ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ whitening å‡ºåŠ›ã«ã™ã‚‹

`PCA(..., whiten=True)` ã‚’æŒ‡å®šã™ã‚‹ã¨ `transform()` ã¯æ—¢å®šã§ whitening ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ã‚’è¿”ã™ã€‚

```python
pca = PCA(n_components=16, whiten=True).fit(X)

Z_white = pca.transform(X)                 # whitened
Z_raw   = pca.transform(X, whiten=False)   # æ˜ç¤ºçš„ã«éwhiten
```

`transform(..., whiten=None)` ã®ã¨ãã¯ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã® `self.whiten` ã«å¾“ã†ã€‚

##### 3.2 æ˜ç¤ºçš„ã« whiten / unwhiten ã™ã‚‹

```python
Z = pca.transform(X, whiten=False)
Z_white = pca.whiten_scores(Z)          # Z_white = Z / sqrt(lambda)
Z_back  = pca.unwhiten_scores(Z_white)  # Z = Z_white * sqrt(lambda)
```

##### 3.3 whitening ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ã‚’é€†å¤‰æ›ã™ã‚‹

`inverse_transform()` ã¯ã€Œå…¥åŠ› Z ãŒ whitened ã‹ã€ã‚’ `whitened` å¼•æ•°ã§è§£é‡ˆã™ã‚‹ã€‚`None` ã®å ´åˆã¯ `self.whiten` ã‚’å‚ç…§ã™ã‚‹ã€‚

```python
# Z_white ãŒ whitened ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤º
X_hat = pca.inverse_transform(Z_white, whitened=True)
```

#### 4. å–å¾—ã§ãã‚‹é‡ï¼ˆãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ / è¨˜å·ãƒ“ãƒ¥ãƒ¼ï¼‰

##### 4.1 ä¸»è¦ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼ˆå­¦ç¿’å¾Œï¼‰

* `mu` : å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« Î¼ï¼ˆcenter=False ã®å ´åˆã¯ 0 ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã€‚
* `W` : ä¸»æˆåˆ†æ–¹å‘ `W=V_k`ï¼ˆshape `(d, k)`ï¼‰ã€‚
* `Z` : å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚³ã‚¢ï¼ˆstore_data=True ã®ã¨ãã®ã¿ä¿æŒï¼‰ã€‚
* `S` : å…±åˆ†æ•£ `S=(X_c^T X_c)/(N - cov_ddof)` ã‚’ **SVDã‹ã‚‰å†æ§‹æˆ**ã—ã¦è¿”ã™ï¼ˆåˆ†æ¯ `N - cov_ddof`ï¼‰ã€‚
* `lambda_k` : ä¿æŒ k æˆåˆ†ã®èª¬æ˜åˆ†æ•£ `Î»`ï¼ˆshape `(k,)`ã€åˆ†æ¯ `N - cov_ddof`ï¼‰ã€‚
* `EVR_within_k` : `Î»_m / sum_{j<=k} Î»_j`ã€‚
* `EVR_total` : `Î»_m / sum_{j<=r} Î»_j`ï¼ˆrank=r ã¾ã§ã®ç·åˆ†æ•£ã§æ­£è¦åŒ–ï¼‰ã€‚

èª¬æ˜åˆ†æ•£æ¯”ã¯ `explained_variance_ratio(mode=...)` ã§ã‚‚å–å¾—ã§ãã‚‹ã€‚

```python
evr_k = pca.explained_variance_ratio(mode="within_k")
evr_total = pca.explained_variance_ratio(mode="total")
```

##### 4.2 è¨˜å·ä¸€å¼ã‚’ã¾ã¨ã‚ã¦å–ã‚Šå‡ºã™ï¼ˆ`symbols`ï¼‰

ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¨˜å·ï¼ˆÎ¼, X_c, S, Q, Î›, U, Ïƒ, Î£, V, W, Z, Î», EVR...ï¼‰ã‚’ **è–„ã„ãƒ“ãƒ¥ãƒ¼**ã¨ã—ã¦ã¾ã¨ã‚ã¦è¿”ã™ã€‚è¤‡è£½ã¯ã›ãšå‚ç…§ã‚’è¿”ã™ã€‚

```python
sym = pca.symbols
print(sym.mu.shape, sym.W.shape, sym.lambda_.shape)
```

æ³¨æ„ï¼š`store_data=False` ã®å ´åˆ `X_c` ã¨ `Z` ã¯ `None` ã«ãªã‚‹ï¼ˆä¿æŒã—ãªã„ï¼‰ã€‚

#### 5. `store_data` ã®è¨­è¨ˆæ„å›³

`store_data=True`ï¼ˆæ—¢å®šï¼‰ã§ã¯ã€æ•°å¼æ¤œè¨¼æ™‚ã®å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«å­¦ç¿’æ™‚ã® `X_c` ã¨ **éwhitenã®** `Z` ã‚’ä¿æŒã™ã‚‹ã€‚

* å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã¯ `Z` ã‚’å†åˆ©ç”¨ã§ãã€`fit_transform()` ãŒé«˜é€ŸåŒ–ã•ã‚Œã‚‹ã€‚
* ãƒ¡ãƒ¢ãƒªç¯€ç´„ã—ãŸã„å ´åˆã¯ `store_data=False` ã«ã—ã¦ä¿æŒã‚’é¿ã‘ã‚‹ï¼ˆãã®å ´åˆ `pca.X_c` / `pca.Z` ã¯ä¾‹å¤–ï¼‰ã€‚

#### 6. device / dtype ã®ä½¿ã„æ–¹ï¼ˆç‰¹ã« `device=int(CUDA index)`ï¼‰

##### 6.1 `device` ã¯ `torch.device | str | int` ã‚’å—ã‘ã‚‹

* `device="cpu"` ã‚„ `device="cuda:0"` ã®ã‚ˆã†ãªæ–‡å­—åˆ—ã¯ `torch.device` ã«å¤‰æ›ã•ã‚Œã‚‹ã€‚
* `device=0` ã®ã‚ˆã†ãª **int ã¯ CUDA index** ã‚’æ„å‘³ã—ã€`cuda:{index}` ã«æ­£è¦åŒ–ã•ã‚Œã‚‹ã€‚CUDA ãŒä½¿ãˆãªã„ç’°å¢ƒã‚„ç¯„å›²å¤– index ã¯ Fail Fast ã§ä¾‹å¤–ã€‚

```python
pca = PCA(n_components=32, device=0, dtype=torch.float32)  # cuda:0 ã«æ­£è¦åŒ–
pca.fit(X_cpu)  # å…¥åŠ›ã‚‚å†…éƒ¨ã§ dtype/device ã«å¯„ã›ã‚‰ã‚Œã‚‹
```

å®Ÿè£…ä¸Šã€å…¥åŠ› `X` / `Z` ã¯ `_as_float_tensor_2d` ã‚’é€šã—ã€æŒ‡å®š `dtype` ã¨ `device` ã«æƒãˆã‚‹ï¼ˆç§»å‹•ãƒ»å‹å¤‰æ›ï¼‰ã€‚

#### 7. `differentiable=True` ã®ä½¿ã„æ–¹ï¼ˆå‹¾é…ã‚°ãƒ©ãƒ•ä¿æŒï¼‰

æ—¢å®šã§ã¯ `fit()` ã¯ `torch.no_grad()` ã§å®Ÿè¡Œã•ã‚Œã‚‹ï¼ˆ= PCA ã®å­¦ç¿’ã‚’è¨ˆç®—ã‚°ãƒ©ãƒ•ã«è¼‰ã›ãªã„ï¼‰ã€‚
`differentiable=True` ã‚’æŒ‡å®šã™ã‚‹ã¨ `fit()` ãŒ `torch.enable_grad()` ã§å®Ÿè¡Œã•ã‚Œã€ã€ŒSVDã‚’å«ã‚€ fit è¨ˆç®—ã€ã‚’ã‚°ãƒ©ãƒ•ã¨ã—ã¦ä¿æŒã—å¾—ã‚‹ã€‚

```python
X = torch.randn(200, 32, requires_grad=True)

pca = PCA(n_components=8, differentiable=True).fit(X)
W = pca.W  # W ãŒ X ã«ä¾å­˜ã—ã†ã‚‹ï¼ˆSVDã®å¾®åˆ†å¯èƒ½é ˜åŸŸå†…ã§ï¼‰

loss = (W ** 2).sum()
loss.backward()
print(X.grad is None)  # False ã«ãªã‚Šå¾—ã‚‹
```

æ³¨æ„ï¼ˆé‹ç”¨ä¸Šã®å‰æãƒ»åˆ¶ç´„ï¼‰

* `torch.linalg.svd` ã®å¾®åˆ†ã¯ã€ç‰¹ç•°å€¤ãŒè¿‘ã„/é‡è¤‡ã™ã‚‹ã‚±ãƒ¼ã‚¹ã§æ•°å€¤çš„ã«ä¸å®‰å®šã«ãªã‚Šå¾—ã‚‹ã€‚ã—ãŸãŒã£ã¦ **é€šå¸¸ç”¨é€”ã§ã¯ `differentiable=False` æ¨å¥¨**ã¨ã„ã†ã‚¹ã‚¿ãƒ³ã‚¹ãŒ docstring ã«æ˜è¨˜ã•ã‚Œã¦ã„ã‚‹ã€‚

#### 8. å…¥åŠ›åˆ¶ç´„ï¼ˆFail Fastï¼‰

* `fit/transform/inverse_transform/whiten_scores/...` ã¯åŸºæœ¬çš„ã« **2æ¬¡å…ƒãƒ»float Tensor** ã‚’è¦æ±‚ã—ã€é•åæ™‚ã¯ `TypeError` / `ValueError` ã‚’å³æ™‚ã«æŠ•ã’ã‚‹ã€‚
* `n_components=k` ã¯ `1 <= k <= min(N, d)` ã‚’è¦æ±‚ã—ã€ã•ã‚‰ã«æ•°å€¤ãƒ©ãƒ³ã‚¯ `r` æ¨å®šå¾Œã«ã¯ `k <= r` ã‚’è¦æ±‚ã™ã‚‹ã€‚
* `cov_ddof=1` ã®ã¨ã **`N >= 2`**ï¼ˆåˆ†æ¯ `N-1` ãŒæ­£ï¼‰ã‚’è¦æ±‚ã™ã‚‹ã€‚`N=1` ã¯ã‚¨ãƒ©ãƒ¼ã€‚
* ä¸­å¿ƒåŒ–å¾Œã«å®šæ•°ï¼ˆ`r=0`ï¼‰ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã€‚

#### 9. å…¸å‹ãƒ¬ã‚·ãƒ”é›†

##### 9.1 ã€Œå­¦ç¿’ã—ãŸ PCA ã‚’åˆ¥ãƒ‡ãƒ¼ã‚¿ã«é©ç”¨ã€

```python
pca = PCA(n_components=16, center=True, store_data=False).fit(X_train)
Z_test = pca.transform(X_test)
```

##### 9.2 ã€Œã‚¹ã‚³ã‚¢ç©ºé–“ã§å‡¦ç†ã—ã¦ã‹ã‚‰å…ƒç©ºé–“ã«æˆ»ã™ã€

```python
Z = pca.transform(X)
Z_filtered = Z.clone()
Z_filtered[:, 4:] = 0.0  # ä¸Šä½4æ¬¡å…ƒã ã‘æ®‹ã™ï¼ˆä¾‹ï¼‰
X_hat = pca.inverse_transform(Z_filtered)
```

##### 9.3 ã€Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨ã«è¨˜å·ã‚’ä¸€æ‹¬ã§æŒã£ã¦ãã‚‹ã€

```python
sym = pca.symbols
# sym.S, sym.Q, sym.Lambda ãªã©ã‚’ãã®ã¾ã¾æ•°å¼æ¤œè¨¼ãƒ»å¯è¦–åŒ–ã«ä½¿ã†
```

---

### å®Ÿè£…

ä»¥ä¸‹ã®ç’°å¢ƒã§ãƒ†ã‚¹ãƒˆæ¸ˆã¿
[![CI](https://github.com/Mantis-Ryuji/zenn-docs/actions/workflows/ci.yml/badge.svg)](https://github.com/Mantis-Ryuji/zenn-docs/actions/workflows/ci.yml)

- `ubuntu-latest`
- `python-version: ["3.10", "3.11", "3.12"]`
- `torch>=2.2,<3`

```python
# pyright: reportConstantRedefinition=false
from __future__ import annotations

from dataclasses import dataclass
from typing import Literal, TypeAlias

import torch

Tensor: TypeAlias = torch.Tensor

__all__ = ["PCA", "PCASymbols"]


def _normalize_device(device: object) -> torch.device:
    """device æŒ‡å®šã‚’ torch.device ã«æ­£è¦åŒ–ã™ã‚‹ï¼ˆFail Fastï¼‰ã€‚"""
    if isinstance(device, torch.device):
        return device
    if isinstance(device, str):
        return torch.device(device)
    # bool ã¯ int ã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹ãªã®ã§å…ˆã«å¼¾ã
    if isinstance(device, bool):
        raise TypeError("device ã« bool ã¯æŒ‡å®šã§ãã¾ã›ã‚“ã€‚")
    if isinstance(device, int):
        if not torch.cuda.is_available():
            raise ValueError("device ã« int (CUDA index) ã‚’æŒ‡å®šã—ã¾ã—ãŸãŒã€CUDA ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        n = torch.cuda.device_count()
        if not (0 <= device < n):
            raise ValueError(f"CUDA index out of range: {device} (device_count={n})")
        return torch.device(f"cuda:{device}")
    raise TypeError(f"Unsupported device type: {type(device)}")


def _as_float_tensor_2d(
    x: object,
    *,
    dtype: torch.dtype | None,
    device: torch.device | str | int | None,
    name: str,
) -> Tensor:
    """å…¥åŠ›ã‚’ 2 æ¬¡å…ƒã®æµ®å‹•å°æ•°ç‚¹ Tensor ã¨ã—ã¦æ¤œè¨¼ãƒ»æ•´å½¢ã™ã‚‹ï¼ˆFail Fastï¼‰ã€‚

    Parameters
    ----------
    x : object
        å…¥åŠ›ã€‚torch.Tensor ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    dtype : torch.dtype | None
        æŒ‡å®šæ™‚ã€dtype ã«å¤‰æ›ã™ã‚‹ã€‚
    device : torch.device | str | int | None
        æŒ‡å®šæ™‚ã€device ã«ç§»å‹•ã™ã‚‹ã€‚int ã¯ CUDA index ã‚’æ„å‘³ã™ã‚‹ã€‚
    name : str
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç”¨ã„ã‚‹åå‰ã€‚

    Returns
    -------
    X : torch.Tensor
        å½¢çŠ¶ (N, d) ã® 2 æ¬¡å…ƒæµ®å‹•å°æ•°ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ã€‚

    Raises
    ------
    TypeError
        x ãŒ torch.Tensor ã§ãªã„ã€ã¾ãŸã¯æµ®å‹•å°æ•°ç‚¹ã§ãªã„å ´åˆã€‚
    ValueError
        x ãŒ 2 æ¬¡å…ƒã§ãªã„ã€ã¾ãŸã¯ç©ºã®å ´åˆã€‚
    """
    if not isinstance(x, torch.Tensor):
        raise TypeError(f"{name} ã¯ torch.Tensor ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: got {type(x)}")
    if x.ndim != 2:
        raise ValueError(f"{name} ã¯2æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ« (N, d) ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: shape={tuple(x.shape)}")
    if x.numel() == 0:
        raise ValueError(f"{name} ã¯ç©ºã§ã‚ã£ã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚")
    if not x.is_floating_point():
        raise TypeError(f"{name} ã¯æµ®å‹•å°æ•°ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: dtype={x.dtype}")

    out = x
    if dtype is not None and out.dtype != dtype:
        out = out.to(dtype=dtype)
    if device is not None:
        dev = _normalize_device(device)
        if out.device != dev:
            out = out.to(device=dev)
    return out


def _safe_div(a: Tensor, b: Tensor, eps: float) -> Tensor:
    """ã‚¼ãƒ­å‰²ã‚’é¿ã‘ãŸå®‰å…¨ãªé™¤ç®— a / max(b, eps)ã€‚"""
    return a / b.clamp_min(eps)


def _svd_rank(
    sigma: Tensor,
    *,
    N: int,
    d: int,
    rtol: float,
) -> int:
    """thin SVD ã®ç‰¹ç•°å€¤åˆ—ã‹ã‚‰æ•°å€¤ãƒ©ãƒ³ã‚¯ã‚’æ¨å®šã™ã‚‹ã€‚

    Notes
    -----
    æœ‰é™ç²¾åº¦ã§ã¯ rank åˆ¤å®šãŒä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„ã®ã§ã€æ¬¡ã®å½¢ã®é–¾å€¤ã‚’æ¡ç”¨ã™ã‚‹ã€‚

        tol = rtol * max(N, d) * sigma_max * eps(dtype)

    ã“ã“ã§ eps(dtype) ã¯æ©Ÿæ¢°ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ï¼ˆtorch.finfoï¼‰ã€‚
    """
    if sigma.numel() == 0:
        return 0
    if not sigma.is_floating_point():
        raise TypeError(f"sigma ã¯æµ®å‹•å°æ•°ç‚¹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: dtype={sigma.dtype}")
    if N <= 0 or d <= 0:
        raise ValueError(f"N,d ã¯æ­£ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: N={N}, d={d}")
    if rtol <= 0.0:
        raise ValueError(f"rtol ã¯æ­£ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: rtol={rtol}")

    sigma_max = float(sigma.max())
    finfo = torch.finfo(sigma.dtype)
    tol = float(rtol) * float(max(N, d)) * sigma_max * float(finfo.eps)
    return int((sigma > tol).sum().item())


@dataclass(frozen=True)
class PCASymbols:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¨˜å·ã‚’ã¾ã¨ã‚ã¦å–ã‚Šå‡ºã™ãŸã‚ã®è–„ã„ãƒ“ãƒ¥ãƒ¼ã€‚

    Notes
    -----
    - å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ `PCA` ãŒä¿æŒã™ã‚‹ Tensor å‚ç…§ã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆè¤‡è£½ã—ãªã„ï¼‰ã€‚
    - store_data=False ã®å ´åˆã€X_c ã¨ Z ã¯ None ã«ãªã‚‹ã€‚
    """
    mu: Tensor
    X_c: Tensor | None
    S: Tensor
    Q: Tensor
    Lambda: Tensor
    U: Tensor
    sigma: Tensor
    Sigma: Tensor
    V: Tensor
    W: Tensor
    Z: Tensor | None
    lambda_: Tensor
    EVR_within_k: Tensor
    EVR_total: Tensor


class PCA:
    r"""PCAï¼ˆSVDãƒ™ãƒ¼ã‚¹ï¼‰ã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®šç¾©ã«å¿ å®Ÿã«å®Ÿè£…ã—ãŸ PyTorch ã‚¯ãƒ©ã‚¹ã€‚

    å®šç¾©ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–æ‹ ï¼‰
    ------------------------
    - ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—: :math:`\mathbf{X}\in\mathbb{R}^{N\times d}`
    - å…¨1ãƒ™ã‚¯ãƒˆãƒ«: :math:`\mathbf{1}_N\in\mathbb{R}^{N}`
    - å¹³å‡: :math:`\boldsymbol{\mu}=\frac{1}{N}\mathbf{X}^\top\mathbf{1}_N\in\mathbb{R}^{d}`
    - ä¸­å¿ƒåŒ–: :math:`\mathbf{X}_c=\mathbf{X}-\mathbf{1}_N\boldsymbol{\mu}^\top\in\mathbb{R}^{N\times d}`
    - å…±åˆ†æ•£ï¼ˆåˆ†æ¯ Nï¼‰: :math:`\mathbf{S}=\frac{1}{N}\mathbf{X}_c^\top\mathbf{X}_c\in\mathbb{R}^{d\times d}`
    - thin SVD: :math:`\mathbf{X}_c=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top`
      ï¼ˆ:math:`\mathbf{U}\in\mathbb{R}^{N\times r}`, :math:`\boldsymbol{\Sigma}\in\mathbb{R}^{r\times r}`,
       :math:`\mathbf{V}\in\mathbb{R}^{d\times r}`ï¼‰
    - ä¸»æˆåˆ†æ–¹å‘: :math:`\mathbf{W}=\mathbf{V}_k\in\mathbb{R}^{d\times k}`
    - ã‚¹ã‚³ã‚¢: :math:`\mathbf{Z}=\mathbf{X}_c\mathbf{W}\in\mathbb{R}^{N\times k}=\mathbf{U}_k\boldsymbol{\Sigma}_k`
    - èª¬æ˜åˆ†æ•£ï¼ˆåˆ†æ¯ Nï¼‰: :math:`\lambda_m=\sigma_m^2/N`
    - èª¬æ˜åˆ†æ•£æ¯”ï¼ˆä¿æŒkå†…ã§æ­£è¦åŒ–ï¼‰: :math:`\mathrm{EVR}^{(k)}_m=\lambda_m/\sum_{j=1}^{k}\lambda_j`
    - èª¬æ˜åˆ†æ•£æ¯”ï¼ˆç·åˆ†æ•£ã§æ­£è¦åŒ–ï¼‰: :math:`\mathrm{EVR}^{(\mathrm{total})}_m=\lambda_m/\sum_{j=1}^{r}\lambda_j`
    - whiteningï¼ˆä»»æ„ï¼‰:
      :math:`\mathbf{Z}^{\mathrm{white}}=\mathbf{Z}\mathrm{diag}(\lambda_1^{-1/2},\dots,\lambda_k^{-1/2})`

    Parameters
    ----------
    n_components : int
        ä¸»æˆåˆ†æ•° kã€‚`1 <= k <= min(N, d)`ã€‚
    center : bool, default=True
        True ã®å ´åˆã€ä¸­å¿ƒåŒ–ã‚’è¡Œã†ã€‚False ã®å ´åˆã¯ Î¼=0 ã¨ã¿ãªã—ã€X_c=X ã‚’ç”¨ã„ã‚‹ã€‚
    whiten : bool, default=False
        True ã®å ´åˆã€transform() ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡ºåŠ›ã‚’ whitening ã—ã¦è¿”ã™ã€‚
    store_data : bool, default=True
        True ã®å ´åˆã€X_c ã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ï¼ˆéwhitenï¼‰Z ã‚’ä¿æŒã™ã‚‹ã€‚
    eps : float, default=1e-12
        ã‚¼ãƒ­å‰²å›é¿ç”¨ã®å°å®šæ•°ã€‚
    dtype : torch.dtype | None
        æŒ‡å®šæ™‚ã€å†…éƒ¨ dtype ã«æƒãˆã‚‹ã€‚
    device : torch.device | str | int | None
        æŒ‡å®šæ™‚ã€å†…éƒ¨ device ã«æƒãˆã‚‹ã€‚int ã¯ CUDA index ã‚’æ„å‘³ã™ã‚‹ã€‚
    rank_rtol : float, default=1e-7
        æ•°å€¤ãƒ©ãƒ³ã‚¯æ¨å®šã®ç›¸å¯¾é–¾å€¤ä¿‚æ•°ã€‚
    differentiable : bool, default=False
        True ã®å ´åˆã€fit() ã‚’å«ã‚€è¨ˆç®—ã‚’ no_grad ã§åŒ…ã¾ãªã„ï¼ˆ=å‹¾é…ã‚°ãƒ©ãƒ•ã‚’ä¿æŒã—å¾—ã‚‹ï¼‰ã€‚
        é€šå¸¸ã® PCA ã§ã¯ False æ¨å¥¨ã€‚

    Attributes
    ----------
    k : int
        ä¸»æˆåˆ†æ•°ã€‚
    N, d, r : int | None
        ã‚µãƒ³ãƒ—ãƒ«æ•° / ç‰¹å¾´æ¬¡å…ƒ / æ•°å€¤ãƒ©ãƒ³ã‚¯ã€‚
    mu_, X_c_ : torch.Tensor | None
        Î¼ ã¨ X_cï¼ˆstore_data=True ã®ã¨ã X_c_ ã‚’ä¿æŒï¼‰ã€‚
    U_, sigma_, Sigma_, V_ : torch.Tensor | None
        thin SVD ã® U, Ïƒï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰, Î£ï¼ˆå¯¾è§’è¡Œåˆ—ï¼‰, Vã€‚
    W_ : torch.Tensor | None
        ä¸»æˆåˆ†æ–¹å‘ W=V_kã€‚
    Z_ : torch.Tensor | None
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚³ã‚¢ Zï¼ˆéwhitenã€store_data=True ã®ã¨ãä¿æŒï¼‰ã€‚
    lambda_, EVR_within_k_, EVR_total_ : torch.Tensor | None
        èª¬æ˜åˆ†æ•£ã¨èª¬æ˜åˆ†æ•£æ¯”ï¼ˆkå†…æ­£è¦åŒ– / ç·åˆ†æ•£æ­£è¦åŒ–ï¼‰ã€‚
    """
    # --- public-ish config ---
    k: int
    center: bool
    whiten: bool
    store_data: bool
    eps: float
    rank_rtol: float
    cov_ddof: Literal[0, 1]
    differentiable: bool

    # --- learned ---
    N: int | None
    d: int | None
    r: int | None

    mu_: Tensor | None
    X_c_: Tensor | None

    U_: Tensor | None
    sigma_: Tensor | None
    Sigma_: Tensor | None
    V_: Tensor | None

    W_: Tensor | None
    Z_: Tensor | None

    lambda_: Tensor | None
    EVR_within_k_: Tensor | None
    EVR_total_: Tensor | None

    def __init__(
        self,
        n_components: int,
        *,
        center: bool = True,
        whiten: bool = False,
        store_data: bool = True,
        eps: float = 1e-12,
        dtype: torch.dtype | None = None,
        device: torch.device | str | int | None = None,
        rank_rtol: float = 1e-7,
        cov_ddof: Literal[0, 1] = 1,
        differentiable: bool = False,
    ) -> None:
        if int(n_components) < 1:
            raise ValueError(f"n_components ã¯ 1 ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: got {n_components}")
        if float(eps) <= 0.0:
            raise ValueError(f"eps ã¯æ­£ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: got {eps}")
        if float(rank_rtol) <= 0.0:
            raise ValueError(f"rank_rtol ã¯æ­£ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: got {rank_rtol}")
        if cov_ddof not in (0, 1):
            raise ValueError(f"cov_ddof ã¯ 0 ã¾ãŸã¯ 1 ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: got {cov_ddof}")

        self.k = int(n_components)
        self.center = bool(center)
        self.whiten = bool(whiten)
        self.store_data = bool(store_data)
        self.eps = float(eps)
        self.rank_rtol = float(rank_rtol)
        self.cov_ddof = cov_ddof
        self.differentiable = bool(differentiable)

        self._dtype: torch.dtype | None = dtype
        self._device: torch.device | None = _normalize_device(device) if device is not None else None

        self.N = None
        self.d = None
        self.r = None

        self.mu_ = None
        self.X_c_ = None

        self.U_ = None
        self.sigma_ = None
        self.Sigma_ = None
        self.V_ = None

        self.W_ = None
        self.Z_ = None

        self.lambda_ = None
        self.EVR_within_k_ = None
        self.EVR_total_ = None

        self._fitted = False

    @property
    def fitted(self) -> bool:
        return self._fitted

    def _require_fit(self) -> None:
        if not self._fitted:
            raise RuntimeError("PCAã¯æœªå­¦ç¿’ã§ã™ã€‚fit() ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")

    # ---- symbol-faithful properties ----
    @property
    def mu(self) -> Tensor:
        self._require_fit()
        assert self.mu_ is not None
        return self.mu_

    @property
    def X_c(self) -> Tensor:
        self._require_fit()
        if self.X_c_ is None:
            raise RuntimeError("store_data=False ã®ãŸã‚ X_c ã¯ä¿æŒã—ã¦ã„ã¾ã›ã‚“ã€‚")
        return self.X_c_

    @property
    def U(self) -> Tensor:
        self._require_fit()
        assert self.U_ is not None
        return self.U_

    @property
    def sigma(self) -> Tensor:
        self._require_fit()
        assert self.sigma_ is not None
        return self.sigma_

    @property
    def Sigma(self) -> Tensor:
        self._require_fit()
        assert self.Sigma_ is not None
        return self.Sigma_

    @property
    def V(self) -> Tensor:
        self._require_fit()
        assert self.V_ is not None
        return self.V_

    @property
    def W(self) -> Tensor:
        self._require_fit()
        assert self.W_ is not None
        return self.W_

    @property
    def Z(self) -> Tensor:
        self._require_fit()
        if self.Z_ is None:
            raise RuntimeError(
                "store_data=False ã®ãŸã‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿Zã¯ä¿æŒã—ã¦ã„ã¾ã›ã‚“ã€‚transform(X) ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
            )
        return self.Z_

    @property
    def lambda_k(self) -> Tensor:
        """ä¿æŒ k æˆåˆ†ã®èª¬æ˜åˆ†æ•£ Î»ï¼ˆshape: (k,)ï¼‰ã‚’è¿”ã™ã€‚"""
        self._require_fit()
        assert self.lambda_ is not None
        return self.lambda_

    @property
    def EVR_within_k(self) -> Tensor:
        """ä¿æŒ k å†…ã§æ­£è¦åŒ–ã•ã‚ŒãŸèª¬æ˜åˆ†æ•£æ¯”ï¼ˆshape: (k,)ï¼‰ã€‚"""
        self._require_fit()
        assert self.EVR_within_k_ is not None
        return self.EVR_within_k_

    @property
    def EVR_total(self) -> Tensor:
        """ç·åˆ†æ•£ï¼ˆrank=r ã¾ã§ï¼‰ã§æ­£è¦åŒ–ã•ã‚ŒãŸèª¬æ˜åˆ†æ•£æ¯”ï¼ˆshape: (k,)ï¼‰ã€‚"""
        self._require_fit()
        assert self.EVR_total_ is not None
        return self.EVR_total_

    @property
    def S(self) -> Tensor:
        """å…±åˆ†æ•£ S ã‚’è¿”ã™ï¼ˆSVD ã‹ã‚‰å†æ§‹æˆã€åˆ†æ¯ N-cov_ddofï¼‰ã€‚"""
        self._require_fit()
        assert self.V_ is not None
        assert self.sigma_ is not None
        assert self.N is not None

        denom = float(self.N - int(self.cov_ddof))
        scale = (self.sigma_ * self.sigma_) / denom  # (r,)
        return (self.V_ * scale.unsqueeze(0)) @ self.V_.T  # (d, d)

    @property
    def Q(self) -> Tensor:
        """å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ— Qï¼ˆthin SVD ã§ã¯ Q=Vï¼‰ã€‚"""
        self._require_fit()
        assert self.V_ is not None
        return self.V_

    @property
    def Lambda(self) -> Tensor:
        """å›ºæœ‰å€¤å¯¾è§’è¡Œåˆ— Î›ï¼ˆthin SVD ã§ã¯ r æ¬¡ï¼‰ã€‚"""
        self._require_fit()
        assert self.sigma_ is not None
        assert self.N is not None
        denom = float(self.N - int(self.cov_ddof))
        lam = (self.sigma_ * self.sigma_) / denom
        return torch.diag(lam)

    @property
    def symbols(self) -> PCASymbols:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¨˜å·ä¸€å¼ã‚’ã¾ã¨ã‚ã¦è¿”ã™ã€‚"""
        self._require_fit()
        assert self.mu_ is not None
        assert self.U_ is not None
        assert self.sigma_ is not None
        assert self.Sigma_ is not None
        assert self.V_ is not None
        assert self.W_ is not None
        assert self.lambda_ is not None
        assert self.EVR_within_k_ is not None
        assert self.EVR_total_ is not None

        return PCASymbols(
            mu=self.mu_,
            X_c=self.X_c_ if self.store_data else None,
            S=self.S,
            Q=self.Q,
            Lambda=self.Lambda,
            U=self.U_,
            sigma=self.sigma_,
            Sigma=self.Sigma_,
            V=self.V_,
            W=self.W_,
            Z=self.Z_ if self.store_data else None,
            lambda_=self.lambda_,
            EVR_within_k=self.EVR_within_k_,
            EVR_total=self.EVR_total_,
        )

    # ---- core API ----
    def fit(self, X: Tensor) -> PCA:
        """ä¸­å¿ƒåŒ–ã¨ thin SVD ã«ã‚ˆã‚Š PCA ã‚’å­¦ç¿’ã™ã‚‹ã€‚

        Notes
        -----
        differentiable=Falseï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã®å ´åˆã€fit ã¯ no_grad ã§å®Ÿè¡Œã•ã‚Œã‚‹ã€‚
        """
        ctx = torch.enable_grad() if self.differentiable else torch.no_grad()
        with ctx:
            X_tensor = _as_float_tensor_2d(X, dtype=self._dtype, device=self._device, name="X")
            N, d = int(X_tensor.shape[0]), int(X_tensor.shape[1])

            denom = float(N - int(self.cov_ddof))
            if denom <= 0.0:
                raise ValueError(f"cov_ddof={self.cov_ddof} ã®ãŸã‚åˆ†æ¯ N-cov_ddof ãŒéæ­£ã§ã™: N={N}")

            if self.k > min(N, d):
                raise ValueError(
                    f"n_components(k)={self.k} ã¯ min(N,d)={min(N, d)} ä»¥ä¸‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
                )

            if self.center:
                # Î¼ = mean over rows (shape: (d,))
                mu = X_tensor.mean(dim=0)
                X_c = X_tensor - mu.unsqueeze(0)
            else:
                mu = torch.zeros(d, dtype=X_tensor.dtype, device=X_tensor.device)
                X_c = X_tensor

            # thin SVD: X_c = U diag(Ïƒ) V^T
            U_full, sigma_full, Vh_full = torch.linalg.svd(X_c, full_matrices=False)

            r = _svd_rank(sigma_full, N=N, d=d, rtol=self.rank_rtol)
            if r == 0:
                raise ValueError(
                    "æ•°å€¤ãƒ©ãƒ³ã‚¯ãŒ 0 ã§ã™ã€‚å…¥åŠ›ãŒå®šæ•°ï¼ˆä¸­å¿ƒåŒ–å¾Œã‚¼ãƒ­ï¼‰ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                )

            # r ã¸ãƒˆãƒªãƒ 
            U = U_full[:, :r].contiguous()          # (N, r)
            sigma = sigma_full[:r].contiguous()     # (r,)
            Vh = Vh_full[:r, :].contiguous()        # (r, d)
            V = Vh.T.contiguous()                   # (d, r)
            Sigma = torch.diag(sigma)               # (r, r)

            if self.k > r:
                raise ValueError(
                    f"k={self.k} ã¯ æ•°å€¤ãƒ©ãƒ³ã‚¯ r={r} ä»¥ä¸‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆrank_rtol={self.rank_rtol}ï¼‰ã€‚"
                )

            W = V[:, : self.k].contiguous()         # (d, k)

            # Î»_m = Ïƒ_m^2 / (N - cov_ddof)  ï¼ˆm=1..kï¼‰
            sigma_k = sigma[: self.k]
            lambda_k = (sigma_k * sigma_k) / denom  # (k,)

            # EVR: within-k & total (rank-r)
            sum_k = lambda_k.sum()
            EVR_within_k = _safe_div(lambda_k, sum_k, eps=self.eps)

            lambda_r = (sigma * sigma) / denom  # (r,)
            sum_r = lambda_r.sum()
            EVR_total = _safe_div(lambda_k, sum_r, eps=self.eps)

            # ä¿å­˜
            self.N = N
            self.d = d
            self.r = r

            self.mu_ = mu

            self.U_ = U
            self.sigma_ = sigma
            self.Sigma_ = Sigma
            self.V_ = V

            self.W_ = W

            self.lambda_ = lambda_k
            self.EVR_within_k_ = EVR_within_k
            self.EVR_total_ = EVR_total

            if self.store_data:
                self.X_c_ = X_c
                # Z = U_k Î£_k
                U_k = U[:, : self.k]                           # (N, k)
                Z = U_k * sigma_k.unsqueeze(0)                 # (N, k)
                self.Z_ = Z
            else:
                self.X_c_ = None
                self.Z_ = None

            self._fitted = True
            return self

    def transform(self, X: Tensor, *, whiten: bool | None = None) -> Tensor:
        """ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šä½ k ä¸»æˆåˆ†ã¸å°„å½±ã—ã€ã‚¹ã‚³ã‚¢ Zï¼ˆã¾ãŸã¯ whitening å¾Œï¼‰ã‚’è¿”ã™ã€‚

        Parameters
        ----------
        X : torch.Tensor
            å½¢çŠ¶ (N, d) ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚
        whiten : bool | None, default=None
            - None ã®å ´åˆã€ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã® self.whiten ã«å¾“ã†ã€‚
            - True ã®å ´åˆã€Z ã‚’ whitening ã—ã¦è¿”ã™ã€‚
            - False ã®å ´åˆã€Zï¼ˆéwhitenï¼‰ã‚’è¿”ã™ã€‚

        Returns
        -------
        Z : torch.Tensor
            å½¢çŠ¶ (N, k) ã®ã‚¹ã‚³ã‚¢ã€‚
        """
        self._require_fit()
        assert self.mu_ is not None
        assert self.W_ is not None
        assert self.d is not None

        X_tensor = _as_float_tensor_2d(X, dtype=self.mu_.dtype, device=self.mu_.device, name="X")
        if int(X_tensor.shape[1]) != int(self.d):
            raise ValueError(f"å…¥åŠ› d={int(X_tensor.shape[1])} ãŒå­¦ç¿’æ™‚ d={int(self.d)} ã¨ä¸€è‡´ã—ã¾ã›ã‚“ã€‚")

        if self.center:
            X_c = X_tensor - self.mu_.unsqueeze(0)
        else:
            X_c = X_tensor

        Z = X_c @ self.W_  # (N, k)

        do_whiten = self.whiten if whiten is None else bool(whiten)
        if do_whiten:
            assert self.lambda_ is not None
            Z = self.whiten_scores(Z)
        return Z

    def fit_transform(self, X: Tensor, *, whiten: bool | None = None) -> Tensor:
        """fit(X) ã®å¾Œã« transform(X) ã‚’å®Ÿè¡Œã—ã¦ã‚¹ã‚³ã‚¢ã‚’è¿”ã™ã€‚

        Notes
        -----
        store_data=True ã‹ã¤ whiten=False ã®å ´åˆã€fit() ã§è¨ˆç®—æ¸ˆã¿ã® Z_ ã‚’è¿”ã™ï¼ˆå†è¨ˆç®—ã—ãªã„ï¼‰ã€‚
        """
        self.fit(X)

        do_whiten = self.whiten if whiten is None else bool(whiten)
        if self.store_data and (not do_whiten):
            # fit() å†…ã§éwhitenã® Z ã‚’ä¿å­˜æ¸ˆã¿
            return self.Z

        return self.transform(X, whiten=do_whiten)

    def whiten_scores(self, Z: Tensor) -> Tensor:
        """Z ã‚’ whitening ã™ã‚‹ï¼ˆZ_white = Z / sqrt(lambda)ï¼‰ã€‚

        Parameters
        ----------
        Z : torch.Tensor
            å½¢çŠ¶ (N, k) ã®éwhitenã‚¹ã‚³ã‚¢ã€‚

        Returns
        -------
        Z_white : torch.Tensor
            å½¢çŠ¶ (N, k) ã® whitened ã‚¹ã‚³ã‚¢ã€‚
        """
        self._require_fit()
        assert self.lambda_ is not None

        Z_tensor = _as_float_tensor_2d(Z, dtype=self.lambda_.dtype, device=self.lambda_.device, name="Z")
        if int(Z_tensor.shape[1]) != int(self.k):
            raise ValueError(f"Z ã®åˆ—æ•°={int(Z_tensor.shape[1])} ã¯ k={int(self.k)} ã¨ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")

        denom = torch.sqrt(self.lambda_).unsqueeze(0)  # (1, k)
        return _safe_div(Z_tensor, denom, eps=self.eps)

    def unwhiten_scores(self, Z_white: Tensor) -> Tensor:
        """whitened ã‚¹ã‚³ã‚¢ã‚’ unwhiten ã™ã‚‹ï¼ˆZ = Z_white * sqrt(lambda)ï¼‰ã€‚

        Parameters
        ----------
        Z_white : torch.Tensor
            å½¢çŠ¶ (N, k) ã® whitened ã‚¹ã‚³ã‚¢ã€‚

        Returns
        -------
        Z : torch.Tensor
            å½¢çŠ¶ (N, k) ã®éwhitenã‚¹ã‚³ã‚¢ã€‚
        """
        self._require_fit()
        assert self.lambda_ is not None

        Z_tensor = _as_float_tensor_2d(Z_white, dtype=self.lambda_.dtype, device=self.lambda_.device, name="Z_white")
        if int(Z_tensor.shape[1]) != int(self.k):
            raise ValueError(
                f"Z_white ã®åˆ—æ•°={int(Z_tensor.shape[1])} ã¯ k={int(self.k)} ã¨ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
            )

        scale = torch.sqrt(self.lambda_).unsqueeze(0)  # (1, k)
        return Z_tensor * scale

    def inverse_transform(self, Z: Tensor, *, whitened: bool | None = None) -> Tensor:
        """ã‚¹ã‚³ã‚¢ Z ã‹ã‚‰å…ƒã®ç‰¹å¾´ç©ºé–“ã¸å†æ§‹æˆã™ã‚‹ã€‚

        Parameters
        ----------
        Z : torch.Tensor
            å½¢çŠ¶ (N, k) ã®ã‚¹ã‚³ã‚¢ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ self.whiten ã«å¾“ã†ã€‚
        whitened : bool | None, default=None
            - None ã®å ´åˆã€self.whiten ã‚’å‚ç…§ã—ã¦ã€Œå…¥åŠ› Z ãŒ whitened ã‹ã€ã‚’è§£é‡ˆã™ã‚‹ã€‚
            - True ã®å ´åˆã€Z ã¯ whitened ã¨ã¿ãªã— unwhiten ã—ã¦ã‹ã‚‰é€†å¤‰æ›ã™ã‚‹ã€‚
            - False ã®å ´åˆã€Z ã¯éwhiten ã¨ã¿ãªã™ã€‚

        Returns
        -------
        X_hat : torch.Tensor
            å½¢çŠ¶ (N, d) ã®å†æ§‹æˆã€‚

        Notes
        -----
        - X_hat_c = Z W^T
        - center=True ã®å ´åˆ: X_hat = X_hat_c + 1_N Î¼^T
        """
        self._require_fit()
        assert self.W_ is not None
        assert self.mu_ is not None
        assert self.d is not None
        assert self.lambda_ is not None

        Z_tensor = _as_float_tensor_2d(Z, dtype=self.mu_.dtype, device=self.mu_.device, name="Z")
        if int(Z_tensor.shape[1]) != int(self.k):
            raise ValueError(f"Z ã®åˆ—æ•°={int(Z_tensor.shape[1])} ã¯ k={int(self.k)} ã¨ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")

        is_whitened = self.whiten if whitened is None else bool(whitened)
        if is_whitened:
            Z_tensor = self.unwhiten_scores(Z_tensor)

        X_hat_c = Z_tensor @ self.W_.T  # (N, d)
        if not self.center:
            return X_hat_c

        return X_hat_c + self.mu_.unsqueeze(0)

    def explained_variance_ratio(self, *, mode: Literal["within_k", "total"] = "within_k") -> Tensor:
        """èª¬æ˜åˆ†æ•£æ¯”ã‚’è¿”ã™ã€‚

        Parameters
        ----------
        mode : {"within_k", "total"}, default="within_k"
            - "within_k": ä¿æŒkå†…ã§æ­£è¦åŒ–
            - "total": rank=r ã®ç·åˆ†æ•£ã§æ­£è¦åŒ–

        Returns
        -------
        evr : torch.Tensor
            å½¢çŠ¶ (k,) ã®èª¬æ˜åˆ†æ•£æ¯”ã€‚
        """
        self._require_fit()
        if mode == "within_k":
            return self.EVR_within_k
        if mode == "total":
            return self.EVR_total
        raise ValueError(f"Unsupported mode: {mode}")
```