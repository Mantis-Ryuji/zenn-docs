---
title: "PCAã®æ•°ç†è§£èª¬ã¨å®Ÿè£…(PyTorch)"
emoji: "ğŸ"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["AI", "æ©Ÿæ¢°å­¦ç¿’", "ç·šå½¢ä»£æ•°", "Python", "PyTorch", "æ¬¡å…ƒå‰Šæ¸›"]
published: false
---

## ä¸»æˆåˆ†åˆ†æã®æ•°ç†è§£èª¬ï¼ˆSVDï¼‰

### 1. å‰æ

ã‚µãƒ³ãƒ—ãƒ«æ•° $N\in\mathbb{N}$ã€ç‰¹å¾´æ¬¡å…ƒ $d\in\mathbb{N}$ ã®ãƒ‡ãƒ¼ã‚¿è¡Œåˆ— $\mathbf{X}\in\mathbb{R}^{N\times d}$ ã‚’è€ƒãˆã‚‹ã€‚
ã“ã“ã§ã€$\mathbf{X}$ ã®ç¬¬ $i$ è¡Œã‚’è¡Œãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ $\mathbf{x}_i^\top\in\mathbb{R}^{1\times d}$ ã¨æ›¸ãã€‚

---

### 2. ä¸­å¿ƒåŒ–ï¼ˆå¹³å‡ã‚’å¼•ã„ãŸåº§æ¨™ç³»ã§è€ƒãˆã‚‹ï¼‰

ã¾ãšåˆ—æ–¹å‘ï¼ˆç‰¹å¾´æ–¹å‘ï¼‰ã®å¹³å‡ã‚’å®šç¾©ã™ã‚‹ã€‚å…¨æˆåˆ† 1 ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã‚’ $\mathbf{1}_N\in\mathbb{R}^{N}$ ã¨ã™ã‚‹ã¨ã€åˆ—å¹³å‡ï¼ˆç‰¹å¾´å¹³å‡ï¼‰ã¯

$$
\boldsymbol{\mu}:=\frac{1}{N}\mathbf{1}_N^\top\mathbf{X}\in\mathbb{R}^{1\times d}
$$

ã§ã‚ã‚‹ã€‚ã“ã‚Œã‚’å„ã‚µãƒ³ãƒ—ãƒ«ã«è¤‡è£½ã—ãŸå¹³å‡è¡Œåˆ—ã¯ $\mathbf{1}_N\boldsymbol{\mu}\in\mathbb{R}^{N\times d}$ ã¨ãªã‚‹ã€‚
ã—ãŸãŒã£ã¦ã€ä¸­å¿ƒåŒ–ãƒ‡ãƒ¼ã‚¿ã‚’

$$
\mathbf{X}_c:=\mathbf{X}-\mathbf{1}_N\boldsymbol{\mu}\in\mathbb{R}^{N\times d}
$$

ã§å®šç¾©ã™ã‚‹ã€‚

---

### 3. ä½æ¬¡å…ƒéƒ¨åˆ†ç©ºé–“ã¸ã®å°„å½±ï¼ˆã‚¹ã‚³ã‚¢ã®å®šç¾©ï¼‰

å–ã‚Šå‡ºã™ä¸»æˆåˆ†æ•°ã‚’ $k\in\mathbb{N}$ï¼ˆé€šå¸¸ $k\ll d$ï¼‰ã¨ã™ã‚‹ã€‚PCAã¯ã€$\mathbb{R}^d$ å†…ã® $k$ æ¬¡å…ƒéƒ¨åˆ†ç©ºé–“ã‚’é¸ã³ã€ãã®éƒ¨åˆ†ç©ºé–“ã¸ã®ç›´äº¤å°„å½±ã§ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¾ã™ã‚‹ã€‚

ãã®ãŸã‚ã«ã€åŸºåº•ï¼ˆloadingsï¼‰ã‚’

$$
\mathbf{W}\in\mathbb{R}^{d\times k},\qquad \mathbf{W}^\top\mathbf{W}=\mathbf{I}_k
$$

ã¨ãŠãï¼ˆåˆ—ãŒç›´äº¤æ­£è¦åŸºåº•ï¼‰ã€‚ã“ã®ã¨ãã€ä¸­å¿ƒåŒ–ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚³ã‚¢ï¼ˆä½æ¬¡å…ƒè¡¨ç¾ï¼‰ã‚’

$$
\mathbf{Z}:=\mathbf{X}_c\mathbf{W}\in\mathbb{R}^{N\times k}
$$

ã¨å®šç¾©ã™ã‚‹ã€‚

- **åˆ—ãŒç›´äº¤æ­£è¦åŸºåº•ã¨ã¯**

ä¸»æˆåˆ† $j\in{1,\ldots,k}$ ã«å¯¾å¿œã™ã‚‹ $\mathbf{W}$ ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã‚’ $\mathbf{w}_j\in\mathbb{R}^{d}$ ã¨æ›¸ãã¨ã€

$$
\mathbf{W}=
\left[
\mathbf{w}_1\ \mathbf{w}_2\ \cdots\ \mathbf{w}_k
\right]
$$

ã§ã‚ã‚‹ã€‚æ¡ä»¶ $\mathbf{W}^\top\mathbf{W}=\mathbf{I}_k$ ã¯ã€ä»»æ„ã® $i,j\in{1,\ldots,k}$ ã«ã¤ã„ã¦

$$
\mathbf{w}_i^\top\mathbf{w}_j=\delta_{ij}
$$

ï¼ˆ$\delta_{ij}$ ã¯ã‚¯ãƒ­ãƒãƒƒã‚«ãƒ¼ã®ãƒ‡ãƒ«ã‚¿ï¼‰ã‚’æ„å‘³ã™ã‚‹ã€‚ã™ãªã‚ã¡

* $i=j$ ãªã‚‰ $\mathbf{w}_i^\top\mathbf{w}_i=1$ï¼ˆé•·ã•ãŒ 1ï¼‰
* $i\neq j$ ãªã‚‰ $\mathbf{w}_i^\top\mathbf{w}_j=0$ï¼ˆäº’ã„ã«ç›´äº¤ï¼‰

ã§ã‚ã‚‹ã€‚

---

### 4. $\mathbf{W}$ ã®æœ€é©åŒ–ï¼šå†æ§‹æˆèª¤å·®æœ€å°åŒ–

å°„å½±ã—ãŸ $\mathbf{Z}$ ã‹ã‚‰å…ƒã®ç©ºé–“ã¸æˆ»ã™å†æ§‹æˆã‚’

$$
\hat{\mathbf{X}}:=\mathbf{Z}\mathbf{W}^\top=\mathbf{X}_c\mathbf{W}\mathbf{W}^\top
$$

ã¨ãŠãã€‚PCAã¯ã€ã“ã®å†æ§‹æˆèª¤å·®ã‚’æœ€å°ã«ã™ã‚‹åŸºåº• $\mathbf{W}^\star$ ã‚’æ±‚ã‚ã‚‹å•é¡Œã¨ã—ã¦å®šç¾©ã§ãã‚‹ï¼š

$$
\mathbf{W}^\star
:=
\operatorname*{arg\,min}_{\mathbf{W}\in\mathbb{R}^{d\times k}}
\left\|\mathbf{X}_c-\mathbf{X}_c\mathbf{W}\mathbf{W}^\top\right\|_F^2
\quad\text{s.t.}\quad
\mathbf{W}^\top\mathbf{W}=\mathbf{I}_k
$$

ã“ã“ã§ $\|\cdot\|_F$ ã¯ Frobenius ãƒãƒ«ãƒ ã§ã‚ã‚‹ã€‚

- **Frobeniusãƒãƒ«ãƒ ã¨ã¯**

è¡Œåˆ— $\mathbf{A}\in\mathbb{R}^{N\times M}$ ã®æˆåˆ†ã‚’ $a_{ij}$ ã¨ã™ã‚‹ã¨ã€

$$
\|\mathbf{A}\|_F=\sqrt{\sum_{i=1}^{N}\sum_{j=1}^{M} a_{ij}^2}
$$

ã§ã‚ã‚‹ã€‚ã¾ãŸã€ãƒˆãƒ¬ãƒ¼ã‚¹ $\mathrm{Tr}$ ã‚’ç”¨ã„ã¦

$$
\|\mathbf{A}\|_F^2=\mathrm{Tr}(\mathbf{A}^\top\mathbf{A})
$$

ã¨æ›¸ã‘ã‚‹ï¼ˆå³è¾ºã¯å…¨æˆåˆ†äºŒä¹—å’Œã«ä¸€è‡´ã™ã‚‹ï¼‰ã€‚

---

### 5. truncated SVD ã«ã‚ˆã‚‹è§£æ³•ï¼ˆå›ºæœ‰å€¤åˆ†è§£ã‚ˆã‚Šæ•°å€¤çš„ã«å®‰å®šï¼‰

ä¸­å¿ƒåŒ–ãƒ‡ãƒ¼ã‚¿ $\mathbf{X}_c\in\mathbb{R}^{N\times d}$ ã«å¯¾ã—ã¦ã€ä¸Šä½ $k$ æˆåˆ†ã®ã¿ã‚’ç”¨ã„ã‚‹ truncated SVD ã‚’è€ƒãˆã‚‹ï¼š

$$
\mathbf{X}_c \approx \mathbf{U}_k\boldsymbol{\Sigma}_k\mathbf{V}_k^\top
$$

ã“ã“ã§

* $\mathbf{U}_k\in\mathbb{R}^{N\times k}$ ã¨ $\mathbf{V}_k\in\mathbb{R}^{d\times k}$ ã¯åˆ—ç›´äº¤ã§ã‚ã‚‹ï¼š

$$
\mathbf{U}_k^\top\mathbf{U}_k=\mathbf{I}_k,\qquad
\mathbf{V}_k^\top\mathbf{V}_k=\mathbf{I}_k
$$

* $\boldsymbol{\Sigma}_k\in\mathbb{R}^{k\times k}$ ã¯ç‰¹ç•°å€¤ã®å¯¾è§’è¡Œåˆ—ã§ã‚ã‚‹ï¼š

$$
\boldsymbol{\Sigma}_k=\mathrm{diag}(\sigma_1,\ldots,\sigma_k),\qquad
\sigma_1\ge\sigma_2\ge\cdots\ge\sigma_k>0
$$

#### 5.1 è§£ã®å½¢ï¼š$\mathbf{W}^\star$ ã¯ $\mathbf{V}_k$

PCA ã®æœ€é©åŸºåº•ï¼ˆloadingsï¼‰ã¯

$$
\mathbf{W}^\star=\mathbf{V}_k\in\mathbb{R}^{d\times k}
$$

ã§ä¸ãˆã‚‰ã‚Œã‚‹ã€‚ã™ãªã‚ã¡ã€ç‰¹ç•°å€¤ $\sigma_i$ ãŒå¤§ãã„é †ã«ä¸Šä½ $k$ æœ¬ã®å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸»æˆåˆ†æ–¹å‘ã¨ã—ã¦æ¡ç”¨ã™ã‚Œã°ã‚ˆã„ã€‚


#### 5.2 ã‚¹ã‚³ã‚¢ã®å½¢ï¼š$\mathbf{Z}$ ã¯ã€Œå·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« Ã— ç‰¹ç•°å€¤ã€

$\mathbf{Z}:=\mathbf{X}_c\mathbf{W}^\star$ ã‚ˆã‚Š

$$
\mathbf{Z}=\mathbf{X}_c\mathbf{V}_k\approx\mathbf{U}_k\boldsymbol{\Sigma}_k\mathbf{V}_k^\top\mathbf{V}_k=
\mathbf{U}_k\boldsymbol{\Sigma}_k
\in\mathbb{R}^{N\times k}
$$

ã¨ãªã‚‹ã€‚ã—ãŸãŒã£ã¦ã€ã‚¹ã‚³ã‚¢ã¯ **ã€Œå·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« Ã— ç‰¹ç•°å€¤ã€** ã¨ã—ã¦ç›´æ¥å¾—ã‚‰ã‚Œã‚‹ã€‚

#### 5.3 åˆ†æ•£ï¼ˆå›ºæœ‰å€¤ï¼‰ã¨ç‰¹ç•°å€¤ï¼š$\lambda_i=\sigma_i^2/N$ ã¨å¯„ä¸ç‡
ã“ã®ç¯€ã®ç›®çš„ã¯ã€ä¸»æˆåˆ† $i$ ã®ã€Œåˆ†æ•£ï¼ˆå›ºæœ‰å€¤ï¼‰ã€$\lambda_i$ ãŒã€SVD ã®ç‰¹ç•°å€¤ $\sigma_i$ ã¨

$$
\lambda_i=\frac{\sigma_i^2}{N}
$$

ã§çµã°ã‚Œã€ã•ã‚‰ã«ãã‚ŒãŒã€Œå°„å½±ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ã€ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã“ã¨ã§ã‚ã‚‹ã€‚

##### (1) å…±åˆ†æ•£è¡Œåˆ—ã¨ PCA ã®å›ºæœ‰å€¤å•é¡Œ

ä¸­å¿ƒåŒ–ãƒ‡ãƒ¼ã‚¿ $\mathbf{X}_c\in\mathbb{R}^{N\times d}$ ã«å¯¾ã—ã€å…±åˆ†æ•£è¡Œåˆ—ã‚’

$$
\mathbf{S}:=\frac{1}{N}\mathbf{X}_c^\top\mathbf{X}_c\in\mathbb{R}^{d\times d}
$$

ã¨å®šç¾©ã™ã‚‹ï¼ˆåˆ†æ¯ã‚’ $N$ ã¨ã™ã‚‹æµå„€ï¼‰ã€‚PCA ã¯ $\mathbf{S}$ ã®å›ºæœ‰å€¤å•é¡Œ

$$
\mathbf{S}\mathbf{v}_i=\lambda_i\mathbf{v}_i,\qquad \|\mathbf{v}_i\|_2=1
$$

ã«ã‚ˆã‚Šã€ä¸»æˆåˆ†æ–¹å‘ $\mathbf{v}_i$ ã¨ã€ãã®æ–¹å‘ã®åˆ†æ•£ $\lambda_i$ ã‚’å¾—ã‚‹ã€‚

##### (2) SVD ã‹ã‚‰ $\lambda_i=\sigma_i^2/N$ ã‚’å¾—ã‚‹

full SVD ã‚’

$$\mathbf{X}_c=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top$$

ã¨ã™ã‚‹ï¼ˆ$\boldsymbol{\Sigma}=\mathrm{diag}(\sigma_1,\ldots,\sigma_r)$ï¼‰ã€‚ã“ã®ã¨ã

$$
\mathbf{S}=\frac{1}{N}\mathbf{X}_c^\top\mathbf{X}_c=\frac{1}{N}\mathbf{V}\boldsymbol{\Sigma}^2\mathbf{V}^\top
$$

ãŒæˆã‚Šç«‹ã¤ã€‚ã‚ˆã£ã¦ $\mathbf{S}$ ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã¯ $\mathbf{V}$ ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«ä¸€è‡´ã—ã€å¯¾å¿œã™ã‚‹å›ºæœ‰å€¤ã¯

$$
\lambda_i=\frac{\sigma_i^2}{N}
$$

ã¨ãªã‚‹ã€‚
ï¼ˆtruncated SVD ã®å ´åˆã‚‚ã€ä¸Šä½ $k$ æˆåˆ†ã«ã¤ã„ã¦åŒã˜é–¢ä¿‚ãŒæˆã‚Šç«‹ã¤ã¨è§£é‡ˆã™ã‚Œã°ã‚ˆã„ã€‚ï¼‰

##### (3) å°„å½±ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ãŒ $\lambda_i$ ã«ä¸€è‡´ã™ã‚‹

ä¸»æˆåˆ†æ–¹å‘ $\mathbf{v}_i$ ã¸ã®å°„å½±ã‚¹ã‚³ã‚¢ï¼ˆ1 æ¬¡å…ƒè¡¨ç¾ï¼‰ã‚’

$$
\mathbf{z}_i:=\mathbf{X}_c\mathbf{v}_i\in\mathbb{R}^{N}
$$

ã¨å®šç¾©ã™ã‚‹ã€‚ä¸­å¿ƒåŒ–ã«ã‚ˆã‚Š $\mathbf{z}_i$ ã®å¹³å‡ã¯ 0 ã§ã‚ã‚‹ãŸã‚ã€åˆ†æ•£ã‚’

$$
\mathrm{Var}(\mathbf{z}_i):=\frac{1}{N}\|\mathbf{z}_i\|_2^2
$$

ã¨ãŠãï¼ˆã“ã®å®šç¾©ã¯ä¸Šã§æ¡ç”¨ã—ãŸ $\mathbf{S}$ ã®åˆ†æ¯ $N$ ã¨æ•´åˆã™ã‚‹ï¼‰ã€‚

ã™ã‚‹ã¨

$$
\mathrm{Var}(\mathbf{z}_i)=\frac{1}{N}\mathbf{z}_i^\top\mathbf{z}_i=\frac{1}{N}\left(\mathbf{X}_c\mathbf{v}_i\right)^\top\left(\mathbf{X}_c\mathbf{v}_i\right)=\mathbf{v}_i^\top\mathbf{S}\mathbf{v}_i
$$

ã§ã‚ã‚Šã€ã•ã‚‰ã«å›ºæœ‰å€¤å•é¡Œ $\mathbf{S}\mathbf{v}_i=\lambda_i\mathbf{v}_i$ ã¨ $|\mathbf{v}_i|_2=1$ ã‚ˆã‚Š

$$
\mathrm{Var}(\mathbf{z}_i)=\mathbf{v}_i^\top\mathbf{S}\mathbf{v}_i=\lambda_i
$$

ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚ã—ãŸãŒã£ã¦

$$
\mathrm{Var}(\mathbf{z}_i)=\lambda_i=\frac{\sigma_i^2}{N}
$$

ãŒæˆç«‹ã™ã‚‹ã€‚
è¨€ã„æ›ãˆã‚‹ã¨ã€$\sigma_i^2$ ã¯å°„å½±ã‚¹ã‚³ã‚¢ã®ã€Œç·ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ã«å¯¾å¿œã—ã€ãã® $1/N$ ãŒåˆ†æ•£ã«ãªã‚‹ã€‚

##### (4) å¯„ä¸ç‡ï¼ˆexplained variance ratioï¼‰

å¯„ä¸ç‡ï¼ˆexplained variance ratioï¼‰ã¯ã€åˆ†æ•£ï¼ˆå›ºæœ‰å€¤ï¼‰ã§æ­£è¦åŒ–ã—ãŸæ¯”ã¨ã—ã¦å®šç¾©ã•ã‚Œã‚‹ã€‚ä¸Šä½ $k$ æˆåˆ†ã«å¯¾ã™ã‚‹ **éƒ¨åˆ†å¯„ä¸ç‡**ã¯

$$
\mathrm{EVR}_i^{(k)}:=\frac{\lambda_i}{\sum_{j=1}^{k}\lambda_j}=\frac{\sigma_i^2}{\sum_{j=1}^{k}\sigma_j^2}
\qquad (i=1,\ldots,k)
$$

ã§ã‚ã‚‹ã€‚

#### 5.4 ãªãœã“ã‚ŒãŒæœ€é©è§£ã«ãªã‚‹ã‹ï¼ˆå†æ§‹æˆèª¤å·®æœ€å°åŒ– â‡” åˆ†æ•£æœ€å¤§åŒ–ï¼‰

å°„å½±è¡Œåˆ—ã‚’

$$
\mathbf{P}:=\mathbf{W}\mathbf{W}^\top\in\mathbb{R}^{d\times d}
$$

ã¨ãŠãã¨ã€$\mathbf{P}$ ã¯å¯¾ç§°ã‹ã¤å†ªç­‰

$$
\mathbf{P}^\top=\mathbf{P},\qquad \mathbf{P}^2=\mathbf{P}
$$

ã‚’æº€ãŸã—ã€$\mathrm{rank}(\mathbf{P})=k$ ã®ç›´äº¤å°„å½±ã§ã‚ã‚‹ã€‚

ç›®çš„é–¢æ•°ã¯

$$
\left\|\mathbf{X}_c-\mathbf{X}_c\mathbf{P}\right\|_F^2=\mathrm{Tr}(\mathbf{X}_c^\top\mathbf{X}_c)-\mathrm{Tr}(\mathbf{P}\mathbf{X}_c^\top\mathbf{X}_c)
$$

ã¨å¤‰å½¢ã§ãã‚‹ã€‚ç¬¬1é … $\mathrm{Tr}(\mathbf{X}_c^\top\mathbf{X}_c)=|\mathbf{X}_c|_F^2$ ã¯ $\mathbf{W}$ ã«ä¾ã‚‰ãªã„å®šæ•°ãªã®ã§ã€æœ€å°åŒ–ã¯æ¬¡ã®æœ€å¤§åŒ–ã«ç­‰ä¾¡ã§ã‚ã‚‹ï¼š

$$
\mathbf{W}^\star=\operatorname*{arg\,max}_{\mathbf{W}^\top\mathbf{W}=\mathbf{I}_k}
\mathrm{Tr}\!\left(\mathbf{W}^\top \mathbf{X}_c^\top\mathbf{X}_c \mathbf{W}\right)
$$

ã“ã®æœ€å¤§åŒ–ã®è§£ã¯ã€$\mathbf{X}_c^\top\mathbf{X}_c$ ã®å›ºæœ‰å€¤ãŒå¤§ãã„æ–¹å‘ã‚’ä¸Šä½ $k$ æœ¬å–ã‚‹ã“ã¨ã«å¯¾å¿œã™ã‚‹ï¼ˆRayleighâ€“Ritz / Ky Fanï¼‰ã€‚truncated SVD ã¯ãã®ä¸Šä½æˆåˆ†ã‚’ç›´æ¥è¿”ã™ãŸã‚ã€çµå±€ $\mathbf{W}^\star=\mathbf{V}_k$ ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

---

## PCAã® Pytorch å®Ÿè£…ï¼ˆGPUå¯¾å¿œï¼‰

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

#### 1. ç›®çš„

2æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ« `X (N, d)` ã‚’ **SVDãƒ™ãƒ¼ã‚¹PCA**ã§ `k=n_components` æ¬¡å…ƒã¸å°„å½±ã—ã€

* ä¸»æˆåˆ†æ–¹å‘ï¼ˆloadingsï¼‰
* ä¸»æˆåˆ†å¾—ç‚¹ï¼ˆscoresï¼‰
* åˆ†æ•£ãƒ»å¯„ä¸ç‡

ã‚’å–å¾—ã™ã‚‹ã€‚
ã“ã®å®Ÿè£…ã¯ `torch.linalg.svd` ã‚’ç”¨ã„ã‚‹ãŸã‚ã€**GPUãƒ†ãƒ³ã‚½ãƒ«ã‚’æ¸¡ã›ã°GPUä¸Šã§è¨ˆç®—**ã•ã‚Œã‚‹ã€‚


#### 2. åŸºæœ¬ä½¿ç”¨ä¾‹ï¼ˆfit â†’ transformï¼‰

ä»¥ä¸‹ã§å®Ÿè£…ã—ãŸ pca.py ã‚’åŒéšå±¤ã«ç½®ãå‰æï¼š

```python
import torch
from pca import PCA

# ä¾‹: GPU
device = "cuda" if torch.cuda.is_available() else "cpu"

X = torch.randn(1000, 256, device=device)

pca = PCA(n_components=32, center=True, whiten=False)
pca.fit(X)

Z = pca.transform(X)          # (1000, 32)
W = pca.loadings              # (256, 32)  ä¸»è»¸ï¼ˆV_kï¼‰
mu = pca.mean                 # (256,)     å¹³å‡
ev = pca.explained_variance_  # (32,)      Î»_i = Ïƒ_i^2 / N
```

* `center=True` ã®ã¨ã `X` ã‚’ç‰¹å¾´é‡å¹³å‡ã§ä¸­å¿ƒåŒ–ã—ã¦ã‹ã‚‰PCAã™ã‚‹ã€‚
* `loadings`ï¼ˆ= `loadings_`ï¼‰ã¯ `(d,k)` ã®åˆ—æ­£è¦ç›´äº¤ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆ`V_k`ï¼‰ã€‚
* `Z = (X - mean) @ loadings` ãŒ transform ã®ä¸­èº«ã€‚

#### 3. fit_transformï¼ˆã¾ã¨ã‚ã¦å®Ÿè¡Œï¼‰

```python
Z = PCA(n_components=32).fit_transform(X)
```

`fit()` ã®å¾Œã«åŒã˜ `X` ã‚’ `transform()` ã™ã‚‹ã ã‘ã€‚

#### 4. whiteningï¼ˆä»»æ„ï¼‰

`whiten=True` ã«ã™ã‚‹ã¨ã€transformå¾Œã«å„æˆåˆ†ã‚’ `sqrt(explained_variance_)` ã§å‰²ã‚‹ï¼š

```python
pca = PCA(n_components=32, whiten=True).fit(X)
Z_white = pca.transform(X)
```

#### 5. ä¿å­˜ã¨å¾©å…ƒï¼ˆsave / loadï¼‰

- **ä¿å­˜**

```python
pca.save("pca.pt")
```

- **èª­ã¿è¾¼ã¿ï¼ˆCPUã¸ï¼‰**

```python
pca2 = PCA.load("pca.pt", map_location="cpu")
```

- **èª­ã¿è¾¼ã¿ï¼ˆGPUã¸æ˜ç¤ºçš„ã«ç§»ã™ï¼‰**

```python
pca2 = PCA.load("pca.pt", map_location="cpu", device="cuda")
```

* `save()` ã¯å­¦ç¿’æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’CPUã¸è½ã¨ã—ã¦ `.pt` ã«ä¿å­˜ã™ã‚‹ã€‚
* `load()` ã¯ `map_location` ã§èª­ã¿è¾¼ã¿ä½ç½®ã‚’æ±ºã‚ã€`device` æŒ‡å®šãŒã‚ã‚Œã°æœ€çµ‚çš„ã«ãã“ã¸ç§»ã™ï¼ˆå„ªå…ˆï¼‰ã€‚

#### 6. ä¸»è¦å±æ€§ï¼ˆå­¦ç¿’å¾Œã«å‚ç…§å¯èƒ½ï¼‰

å­¦ç¿’å¾Œã€ä»¥ä¸‹ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ï¼š

* `pca.mean_` / `pca.mean`ï¼šç‰¹å¾´é‡å¹³å‡ `(d,)`
* `pca.loadings_` / `pca.loadings`ï¼šä¸»è»¸ `(d,k)`
* `pca.components_`ï¼š`loadings_` ã®åˆ¥åï¼ˆsklearnäº’æ›ï¼‰
* `pca.singular_values_`ï¼šç‰¹ç•°å€¤ `(k,)`
* `pca.explained_variance_`ï¼šåˆ†æ•£ `(k,)`ï¼ˆã“ã®å®Ÿè£…ã¯ **Ïƒ^2 / N**ï¼‰
* `pca.explained_variance_ratio_`ï¼šå¯„ä¸ç‡ `(k,)`ï¼ˆ**ä¸Šä½kæˆåˆ†å†…ã§æ­£è¦åŒ–**ï¼‰
* `pca.n_samples_`ï¼šå­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°
* `pca.fitted_`ï¼šå­¦ç¿’æ¸ˆã¿ãƒ•ãƒ©ã‚°

#### 7. å…¥åŠ›åˆ¶ç´„ï¼ˆFail Fastï¼‰

* `X` ã¯ **torch.Tensor ã‹ã¤ float**ï¼ˆæ•´æ•°ã¯ä¸å¯ï¼‰
* `X.ndim == 2`ï¼ˆå½¢çŠ¶ã¯ `(N, d)`ï¼‰
* `transform()` ã® `d` ã¯ fitæ™‚ã¨ä¸€è‡´å¿…é ˆ
  ã“ã‚Œã‚‰ã«é•åã™ã‚‹ã¨ `TypeError` / `ValueError` ã§å³è½ã¡ã‚‹ã€‚

---

### å®Ÿè£… (`pca.py`)

```python
from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, Optional, TypeAlias, Union

import torch

Tensor: TypeAlias = torch.Tensor
PathLike: TypeAlias = Union[str, Path]

__all__ = ["PCA"]


def _as_tensor_2d(x: Any, *, dtype: Optional[torch.dtype], device: Optional[torch.device]) -> Tensor:
    """å…¥åŠ›ã‚’2æ¬¡å…ƒfloat Tensorã¨ã—ã¦æ¤œè¨¼ãƒ»æ•´å½¢ã™ã‚‹ï¼ˆFail Fastï¼‰ã€‚

    Parameters
    ----------
    x : Any
        å…¥åŠ›ã€‚torch.Tensor ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    dtype : torch.dtype, optional
        æŒ‡å®šæ™‚ã€dtype ã«å¤‰æ›ã™ã‚‹ã€‚
    device : torch.device, optional
        æŒ‡å®šæ™‚ã€device ã«ç§»å‹•ã™ã‚‹ã€‚

    Returns
    -------
    X : torch.Tensor
        å½¢çŠ¶ (N, d) ã®2æ¬¡å…ƒæµ®å‹•å°æ•°ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ã€‚

    Raises
    ------
    TypeError
        x ãŒ torch.Tensor ã§ãªã„ã€ã¾ãŸã¯æµ®å‹•å°æ•°ç‚¹ã§ãªã„å ´åˆã€‚
    ValueError
        x ãŒ2æ¬¡å…ƒã§ãªã„ã€ã¾ãŸã¯ç©ºã®å ´åˆã€‚
    """
    if not isinstance(x, torch.Tensor):
        raise TypeError(f"X ã¯ torch.Tensor ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: got {type(x)}")
    if x.ndim != 2:
        raise ValueError(f"X ã¯2æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ« (N, d) ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: shape={tuple(x.shape)}")
    if x.numel() == 0:
        raise ValueError("X ã¯ç©ºã§ã‚ã£ã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚")
    if not x.is_floating_point():
        raise TypeError(f"X ã¯æµ®å‹•å°æ•°ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: dtype={x.dtype}")

    if dtype is not None and x.dtype != dtype:
        x = x.to(dtype=dtype)
    if device is not None and x.device != device:
        x = x.to(device=device)
    return x


def _safe_div(a: Tensor, b: Tensor, eps: float) -> Tensor:
    """ã‚¼ãƒ­å‰²ã‚’é¿ã‘ãŸå®‰å…¨ãªé™¤ç®— a / max(b, eps)ã€‚"""
    return a / b.clamp_min(eps)


class PCA:
    r"""PyTorchå®Ÿè£…ã®PCAï¼ˆSVDãƒ™ãƒ¼ã‚¹ã€GPUå¯¾å¿œï¼‰ã€‚

    æœ¬ã‚¯ãƒ©ã‚¹ã¯æ¬¡ã®å®šå¼åŒ–ã«å³å¯†ã«å¾“ã†ã€‚

    **ä¸­å¿ƒåŒ–**
    - å…¥åŠ›è¡Œåˆ—ã‚’ :math:`\mathbf{X}\in\mathbb{R}^{N\times d}` ã¨ã™ã‚‹ã€‚
    - ç‰¹å¾´é‡å¹³å‡ :math:`\boldsymbol{\mu}\in\mathbb{R}^{d}` ã‚’ç”¨ã„ã¦
      :math:`\mathbf{X}_c = \mathbf{X} - \mathbf{1}\boldsymbol{\mu}^{\top}` ã¨ã™ã‚‹ã€‚

    **ï¼ˆæ‰“ã¡åˆ‡ã‚Šï¼‰SVD**
    - :math:`\mathbf{X}_c \approx \mathbf{U}_k \boldsymbol{\Sigma}_k \mathbf{V}_k^{\top}`

    **ä¸»è»¸ï¼ˆloadingsï¼‰ã¨ã‚¹ã‚³ã‚¢ï¼ˆscoresï¼‰**
    - loadingsï¼ˆä¸»æˆåˆ†æ–¹å‘ï¼‰: :math:`\mathbf{W}=\mathbf{V}_k \in \mathbb{R}^{d\times k}`
    - scoresï¼ˆä¸»æˆåˆ†å¾—ç‚¹ï¼‰: :math:`\mathbf{Z}=\mathbf{X}_c\mathbf{W}\in\mathbb{R}^{N\times k}`

    Parameters
    ----------
    n_components : int
        ä¸»æˆåˆ†æ•° :math:`k`ã€‚
    center : bool, default=True
        True ã®å ´åˆã€ç‰¹å¾´é‡å¹³å‡ã‚’å¼•ã„ã¦ã‹ã‚‰SVDã‚’è¡Œã†ã€‚
    whiten : bool, default=False
        True ã®å ´åˆã€transform å‡ºåŠ› :math:`\mathbf{Z}` ã‚’
        :math:`\sqrt{\lambda_i}`ï¼ˆæˆåˆ†åˆ†æ•£ï¼‰ã§å‰²ã‚Šã€å„æˆåˆ†ã‚’å˜ä½åˆ†æ•£åŒ–ã™ã‚‹ã€‚
    eps : float, default=1e-12
        æ•°å€¤å®‰å®šåŒ–ï¼ˆã‚¼ãƒ­å‰²å›é¿ï¼‰ç”¨ã®å°ã•ã„å®šæ•°ã€‚
    dtype : torch.dtype, optional
        fit/transform æ™‚ã«å†…éƒ¨ã§ã“ã®dtypeã¸æƒãˆã‚‹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¥åŠ›ã«å¾“ã†ï¼‰ã€‚
    device : str | torch.device, optional
        fit/transform æ™‚ã«å†…éƒ¨ã§ã“ã®deviceã¸æƒãˆã‚‹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¥åŠ›ã«å¾“ã†ï¼‰ã€‚

    Attributes
    ----------
    mean_ : torch.Tensor
        ç‰¹å¾´é‡å¹³å‡ :math:`\boldsymbol{\mu}`ã€‚å½¢çŠ¶ (d,)ã€‚
        center=False ã®å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã€‚
    loadings_ : torch.Tensor
        loadingsï¼ˆä¸»è»¸ï¼‰: :math:`\mathbf{W}`ã€‚å½¢çŠ¶ (d, k)ã€‚
        å„åˆ—ã¯ç›´äº¤è¦æ ¼åŒ–ã•ã‚Œã‚‹ã€‚
    components_ : torch.Tensor
        sklearnäº’æ›ã®åˆ¥åã€‚loadings_ ã¨åŒä¸€å‚ç…§ã€‚
    singular_values_ : torch.Tensor
        ç‰¹ç•°å€¤ :math:`(\sigma_1,\ldots,\sigma_k)`ã€‚å½¢çŠ¶ (k,)ã€‚
    explained_variance_ : torch.Tensor
        å„æˆåˆ†ã®åˆ†æ•£ :math:`\lambda_i`ã€‚å½¢çŠ¶ (k,)ã€‚
        æœ¬å®Ÿè£…ã§ã¯ **åˆ†æ¯N** ã®æµå„€ã§ :math:`\lambda_i=\sigma_i^2 / N` ã‚’æ¡ç”¨ã™ã‚‹ã€‚
    explained_variance_ratio_ : torch.Tensor
        ä¿æŒã—ãŸkæˆåˆ†å†…ã§æ­£è¦åŒ–ã—ãŸå¯„ä¸ç‡ã€‚
        :math:`\lambda_i / \sum_{j=1}^{k}\lambda_j`ã€‚å½¢çŠ¶ (k,)ã€‚
    n_samples_ : int
        fit ã«ç”¨ã„ãŸã‚µãƒ³ãƒ—ãƒ«æ•° :math:`N`ã€‚
    fitted_ : bool
        å­¦ç¿’æ¸ˆã¿ãƒ•ãƒ©ã‚°ã€‚

    Notes
    -----
    - SVDã¯ `torch.linalg.svd(Xc, full_matrices=False)` ã‚’ç”¨ã„ã‚‹ã€‚
      ã“ã‚Œã¯å³å¯†ã ãŒã€å¤§è¦æ¨¡ (N,dãŒéå¸¸ã«å¤§ãã„) ã§ã¯ãƒ¡ãƒ¢ãƒª/è¨ˆç®—ãŒé‡ã„ã€‚
    - explained_variance_ratio_ ã¯ã€Œä¸Šä½kå†…ã§ã®æ­£è¦åŒ–ã€ã§ã‚ã‚‹ç‚¹ã«æ³¨æ„ã€‚
      å…¨æˆåˆ†ï¼ˆrankå…¨ä½“ï¼‰ã§ã®EVRãŒå¿…è¦ãªã‚‰å…¨ç‰¹ç•°å€¤ãŒå¿…è¦ã€‚
    - PyTorch 2.6 ä»¥é™ã§ã¯ `torch.load` ã® `weights_only` ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãŒ True ã«ãªã£ãŸã€‚
      æœ¬å®Ÿè£…ã¯ checkpoint ã‚’ã€ŒTensor + ãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–ã®ã¿ã€ã®è¾æ›¸ã¨ã—ã¦ä¿å­˜ã—ã€
      `weights_only=True` ã§å®‰å…¨ã«ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹å½¢å¼ã«ã—ã¦ã„ã‚‹ã€‚

    Raises
    ------
    ValueError
        å…¥åŠ›shapeä¸æ­£ã€n_componentsãŒç¯„å›²å¤–ãªã©ã€‚
    TypeError
        å…¥åŠ›ãŒtorch.Tensorã§ãªã„ã€ã¾ãŸã¯æµ®å‹•å°æ•°ç‚¹ã§ãªã„å ´åˆã€‚

    Examples
    --------
    >>> import torch
    >>> from decomposition.pca import PCA
    >>> X = torch.randn(1000, 256, device="cuda")
    >>> pca = PCA(n_components=32).fit(X)
    >>> Z = pca.transform(X)
    >>> pca.save("pca.pt")
    >>> pca2 = PCA.load("pca.pt", map_location="cpu")
    """

    def __init__(
        self,
        n_components: int,
        *,
        center: bool = True,
        whiten: bool = False,
        eps: float = 1e-12,
        dtype: Optional[torch.dtype] = None,
        device: Optional[Union[str, torch.device]] = None,
    ) -> None:
        if not isinstance(n_components, int) or n_components <= 0:
            raise ValueError(f"n_components ã¯æ­£ã®intã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: got {n_components}")
        if not isinstance(center, bool):
            raise TypeError("center ã¯ bool ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
        if not isinstance(whiten, bool):
            raise TypeError("whiten ã¯ bool ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
        if not isinstance(eps, (float, int)) or float(eps) <= 0.0:
            raise ValueError(f"eps ã¯æ­£ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: got {eps}")

        self.n_components: int = n_components
        self.center: bool = center
        self.whiten: bool = whiten
        self.eps: float = float(eps)

        self._dtype: Optional[torch.dtype] = dtype
        self._device: Optional[torch.device] = torch.device(device) if device is not None else None

        self.mean_: Optional[Tensor] = None
        self.loadings_: Optional[Tensor] = None
        self.components_: Optional[Tensor] = None
        self.singular_values_: Optional[Tensor] = None
        self.explained_variance_: Optional[Tensor] = None
        self.explained_variance_ratio_: Optional[Tensor] = None
        self.n_samples_: Optional[int] = None
        self.fitted_: bool = False

    @property
    def loadings(self) -> Tensor:
        """loadings_ï¼ˆä¸»è»¸ï¼‰ã¸ã®åˆ¥åã‚¢ã‚¯ã‚»ã‚¹ã€‚"""
        if not self.fitted_ or self.loadings_ is None:
            raise RuntimeError("PCAã¯æœªå­¦ç¿’ã§ã™ã€‚fit() ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")
        return self.loadings_

    @property
    def mean(self) -> Tensor:
        """mean_ï¼ˆç‰¹å¾´é‡å¹³å‡ï¼‰ã¸ã®åˆ¥åã‚¢ã‚¯ã‚»ã‚¹ã€‚"""
        if not self.fitted_ or self.mean_ is None:
            raise RuntimeError("PCAã¯æœªå­¦ç¿’ã§ã™ã€‚fit() ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")
        return self.mean_

    def fit(self, X: Tensor) -> "PCA":
        """PCAã‚’å­¦ç¿’ã™ã‚‹ï¼ˆä¸­å¿ƒåŒ–â†’SVDï¼‰ã€‚

        Parameters
        ----------
        X : torch.Tensor
            å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚å½¢çŠ¶ (N, d) ã®æµ®å‹•å°æ•°ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ã€‚

        Returns
        -------
        self : PCA
            å­¦ç¿’æ¸ˆã¿ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚

        Raises
        ------
        ValueError
            n_components ãŒ min(N, d) ã‚’è¶…ãˆã‚‹å ´åˆãªã©ã€‚
        TypeError
            å…¥åŠ›ãŒtorch.Tensorã§ãªã„ã€ã¾ãŸã¯æµ®å‹•å°æ•°ç‚¹ã§ãªã„å ´åˆã€‚
        """
        X = _as_tensor_2d(X, dtype=self._dtype, device=self._device)
        n, d = int(X.shape[0]), int(X.shape[1])

        k = self.n_components
        if k > min(n, d):
            raise ValueError(f"n_components={k} ã¯ min(N,d)={min(n, d)} ä»¥ä¸‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")

        if self.center:
            mean = X.mean(dim=0)
            Xc = X - mean
        else:
            mean = torch.zeros(d, dtype=X.dtype, device=X.device)
            Xc = X

        # Xc = U S Vh
        _, S, Vh = torch.linalg.svd(Xc, full_matrices=False)

        S_k = S[:k]
        V_k = Vh[:k, :].T.contiguous()  # (d, k)

        explained_var = (S_k * S_k) / float(n)  # lambda_i = sigma_i^2 / N
        evr = _safe_div(explained_var, explained_var.sum(), eps=self.eps)

        self.mean_ = mean
        self.loadings_ = V_k
        self.components_ = self.loadings_
        self.singular_values_ = S_k
        self.explained_variance_ = explained_var
        self.explained_variance_ratio_ = evr
        self.n_samples_ = n
        self.fitted_ = True
        return self

    def transform(self, X: Tensor) -> Tensor:
        """å­¦ç¿’æ¸ˆã¿PCAç©ºé–“ã¸å°„å½±ã™ã‚‹ã€‚

        Parameters
        ----------
        X : torch.Tensor
            å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚å½¢çŠ¶ (N, d)ã€‚

        Returns
        -------
        Z : torch.Tensor
            ä¸»æˆåˆ†å¾—ç‚¹ã€‚å½¢çŠ¶ (N, k)ã€‚
            whiten=True ã®å ´åˆã€å„åˆ—ã‚’ sqrt(explained_variance_) ã§å‰²ã‚‹ã€‚

        Raises
        ------
        RuntimeError
            æœªå­¦ç¿’ã®å ´åˆã€‚
        ValueError
            å…¥åŠ›ã®æ¬¡å…ƒ d ãŒå­¦ç¿’æ™‚ã¨ä¸€è‡´ã—ãªã„å ´åˆã€‚
        """
        if not self.fitted_:
            raise RuntimeError("PCAã¯æœªå­¦ç¿’ã§ã™ã€‚fit() ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")
        assert self.mean_ is not None
        assert self.loadings_ is not None
        assert self.explained_variance_ is not None

        X = _as_tensor_2d(X, dtype=self.mean_.dtype, device=self.mean_.device)
        if int(X.shape[1]) != int(self.mean_.shape[0]):
            raise ValueError(
                f"å…¥åŠ›d={int(X.shape[1])}ãŒå­¦ç¿’æ™‚d={int(self.mean_.shape[0])}ã¨ä¸€è‡´ã—ã¾ã›ã‚“ã€‚"
            )

        Xc = X - self.mean_ if self.center else X
        Z = Xc @ self.loadings_

        if self.whiten:
            Z = _safe_div(Z, torch.sqrt(self.explained_variance_), eps=self.eps)
        return Z

    def fit_transform(self, X: Tensor) -> Tensor:
        """å­¦ç¿’ã¨å¤‰æ›ã‚’ã¾ã¨ã‚ã¦å®Ÿè¡Œã™ã‚‹ã€‚"""
        self.fit(X)
        return self.transform(X)

    def _state_dict(self) -> Dict[str, Any]:
        """å­¦ç¿’æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€ŒTensor + ãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–ã®ã¿ã€ã®è¾æ›¸ã¨ã—ã¦è¿”ã™ã€‚

        Notes
        -----
        PyTorch 2.6 ä»¥é™ã§ã¯ `torch.load(..., weights_only=True)` ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ãªã‚Šã€
        ä»»æ„ã®ã‚¯ãƒ©ã‚¹ï¼ˆdataclass ç­‰ï¼‰ã‚’å«ã‚€pickleã®å¾©å…ƒãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ã€‚
        ãã®ãŸã‚ã€æœ¬å®Ÿè£…ã§ã¯ checkpoint å†…ã«ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å«ã‚ãªã„ã€‚
        """
        if not self.fitted_:
            raise RuntimeError("æœªå­¦ç¿’ã®ãŸã‚ä¿å­˜ã§ãã¾ã›ã‚“ã€‚fit() ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")
        assert self.mean_ is not None
        assert self.loadings_ is not None
        assert self.singular_values_ is not None
        assert self.explained_variance_ is not None
        assert self.explained_variance_ratio_ is not None
        assert self.n_samples_ is not None

        state: Dict[str, Any] = {
            "format_version": 1,
            "n_components": int(self.n_components),
            "center": bool(self.center),
            "whiten": bool(self.whiten),
            "eps": float(self.eps),
            "mean": self.mean_.detach().cpu(),
            "loadings": self.loadings_.detach().cpu(),
            "singular_values": self.singular_values_.detach().cpu(),
            "explained_variance": self.explained_variance_.detach().cpu(),
            "explained_variance_ratio": self.explained_variance_ratio_.detach().cpu(),
            "n_samples": int(self.n_samples_),
            "dtype": str(self.mean_.dtype),
            "device": str(self.mean_.device),
        }
        return {"pca_state": state}

    def save(self, path: PathLike) -> None:
        """å­¦ç¿’æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ã€‚

        Parameters
        ----------
        path : str | pathlib.Path
            ä¿å­˜å…ˆãƒ‘ã‚¹ï¼ˆä¾‹: "pca.pt"ï¼‰ã€‚

        Raises
        ------
        RuntimeError
            æœªå­¦ç¿’ã®å ´åˆã€‚
        """
        p = Path(path)
        p.parent.mkdir(parents=True, exist_ok=True)
        torch.save(self._state_dict(), p)

    @classmethod
    def load(
        cls,
        path: PathLike,
        *,
        map_location: Optional[Union[str, torch.device]] = None,
        device: Optional[Union[str, torch.device]] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> "PCA":
        """ä¿å­˜æ¸ˆã¿PCAã‚’èª­ã¿è¾¼ã‚€ã€‚

        Parameters
        ----------
        path : str | pathlib.Path
            save() ã§ä¿å­˜ã—ãŸ .pt ã¸ã®ãƒ‘ã‚¹ã€‚
        map_location : str | torch.device, optional
            torch.load ã«æ¸¡ã™ map_locationï¼ˆä¾‹: "cpu" / "cuda"ï¼‰ã€‚
        device : str | torch.device, optional
            èª­ã¿è¾¼ã¿å¾Œã«æ˜ç¤ºçš„ã«ç§»ã™deviceã€‚æŒ‡å®šã—ãŸå ´åˆ map_location ã‚ˆã‚Šå„ªå…ˆã€‚
        dtype : torch.dtype, optional
            èª­ã¿è¾¼ã¿å¾Œã«ã“ã®dtypeã¸ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹ã€‚

        Returns
        -------
        pca : PCA
            å¾©å…ƒã•ã‚ŒãŸPCAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚

        Raises
        ------
        FileNotFoundError
            path ãŒå­˜åœ¨ã—ãªã„å ´åˆã€‚
        ValueError
            ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå½¢å¼ãŒä¸æ­£ãªå ´åˆã€‚
        """
        p = Path(path)
        if not p.exists():
            raise FileNotFoundError(str(p))

        # PyTorch 2.6+ ã¯ weights_only=True ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆå®‰å…¨å´ï¼‰ã€‚
        # æœ¬å®Ÿè£…ã¯ dict-only å½¢å¼ã®ãŸã‚ weights_only=True ã§èª­ã¿è¾¼ã‚€ã€‚
        try:
            payload = torch.load(p, map_location=map_location, weights_only=True)
        except TypeError:
            # å¤ã„PyTorchã§ã¯ weights_only å¼•æ•°ãŒå­˜åœ¨ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹
            payload = torch.load(p, map_location=map_location)

        if not isinstance(payload, dict) or "pca_state" not in payload:
            raise ValueError("PCAãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆ'pca_state' ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰ã€‚")

        state = payload["pca_state"]
        if not isinstance(state, dict):
            raise ValueError("PCAãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆpca_state ãŒ dict ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰ã€‚")

        required_keys = (
            "n_components",
            "center",
            "whiten",
            "eps",
            "mean",
            "loadings",
            "singular_values",
            "explained_variance",
            "explained_variance_ratio",
            "n_samples",
        )
        missing = [k for k in required_keys if k not in state]
        if missing:
            raise ValueError(f"PCAãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆæ¬ æã‚­ãƒ¼: {missing}ï¼‰ã€‚")

        pca = cls(
            n_components=int(state["n_components"]),
            center=bool(state["center"]),
            whiten=bool(state["whiten"]),
            eps=float(state["eps"]),
            dtype=dtype,
            device=torch.device(device) if device is not None else None,
        )

        tgt_device: Optional[torch.device]
        if device is not None:
            tgt_device = torch.device(device)
        elif map_location is not None:
            tgt_device = torch.device(map_location)
        else:
            tgt_device = None

        mean = state["mean"]
        loadings = state["loadings"]
        sv = state["singular_values"]
        ev = state["explained_variance"]
        evr = state["explained_variance_ratio"]

        if not isinstance(mean, torch.Tensor) or not isinstance(loadings, torch.Tensor):
            raise ValueError("PCAãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆãƒ†ãƒ³ã‚½ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰ã€‚")

        if dtype is not None:
            mean = mean.to(dtype=dtype)
            loadings = loadings.to(dtype=dtype)
            sv = sv.to(dtype=dtype)
            ev = ev.to(dtype=dtype)
            evr = evr.to(dtype=dtype)

        if tgt_device is not None:
            mean = mean.to(device=tgt_device)
            loadings = loadings.to(device=tgt_device)
            sv = sv.to(device=tgt_device)
            ev = ev.to(device=tgt_device)
            evr = evr.to(device=tgt_device)

        pca.mean_ = mean
        pca.loadings_ = loadings
        pca.components_ = pca.loadings_
        pca.singular_values_ = sv
        pca.explained_variance_ = ev
        pca.explained_variance_ratio_ = evr
        pca.n_samples_ = int(state["n_samples"])
        pca.fitted_ = True
        return pca
```